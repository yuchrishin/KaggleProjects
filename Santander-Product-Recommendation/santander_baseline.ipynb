{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle - Santander Product Recommendation Baseline Modeling\n",
    "**Author: Chris Shin**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Items to do during preprocessing:\n",
    "- impute missing values\n",
    "    - for products, we can replace missing values with 0 indicating the products were not purchased\n",
    "- preprocess categorical and numerical data.\n",
    "    - Categorical data: performs Label Encoding through factorize(). \n",
    "    - Numerical data: converted to integer data for those data type is expressed as an object. \n",
    "- combine train and test data in order to provide same data cleaning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\2022540250.py:7: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trn = pd.read_csv('./data/train_ver2.csv')\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\2022540250.py:8: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tst = pd.read_csv('./data/test_ver2.csv')\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\2022540250.py:33: FutureWarning: Specifying the specific value to use for `na_sentinel` is deprecated and will be removed in a future version of pandas. Specify `use_na_sentinel=True` to use the sentinel value -1, and `use_na_sentinel=False` to encode NaN values.\n",
      "  df[col], _ = df[col].factorize(na_sentinel=-99)\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\2022540250.py:33: FutureWarning: Specifying the specific value to use for `na_sentinel` is deprecated and will be removed in a future version of pandas. Specify `use_na_sentinel=True` to use the sentinel value -1, and `use_na_sentinel=False` to encode NaN values.\n",
      "  df[col], _ = df[col].factorize(na_sentinel=-99)\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\2022540250.py:33: FutureWarning: Specifying the specific value to use for `na_sentinel` is deprecated and will be removed in a future version of pandas. Specify `use_na_sentinel=True` to use the sentinel value -1, and `use_na_sentinel=False` to encode NaN values.\n",
      "  df[col], _ = df[col].factorize(na_sentinel=-99)\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\2022540250.py:33: FutureWarning: Specifying the specific value to use for `na_sentinel` is deprecated and will be removed in a future version of pandas. Specify `use_na_sentinel=True` to use the sentinel value -1, and `use_na_sentinel=False` to encode NaN values.\n",
      "  df[col], _ = df[col].factorize(na_sentinel=-99)\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\2022540250.py:33: FutureWarning: Specifying the specific value to use for `na_sentinel` is deprecated and will be removed in a future version of pandas. Specify `use_na_sentinel=True` to use the sentinel value -1, and `use_na_sentinel=False` to encode NaN values.\n",
      "  df[col], _ = df[col].factorize(na_sentinel=-99)\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\2022540250.py:33: FutureWarning: Specifying the specific value to use for `na_sentinel` is deprecated and will be removed in a future version of pandas. Specify `use_na_sentinel=True` to use the sentinel value -1, and `use_na_sentinel=False` to encode NaN values.\n",
      "  df[col], _ = df[col].factorize(na_sentinel=-99)\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\2022540250.py:33: FutureWarning: Specifying the specific value to use for `na_sentinel` is deprecated and will be removed in a future version of pandas. Specify `use_na_sentinel=True` to use the sentinel value -1, and `use_na_sentinel=False` to encode NaN values.\n",
      "  df[col], _ = df[col].factorize(na_sentinel=-99)\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\2022540250.py:33: FutureWarning: Specifying the specific value to use for `na_sentinel` is deprecated and will be removed in a future version of pandas. Specify `use_na_sentinel=True` to use the sentinel value -1, and `use_na_sentinel=False` to encode NaN values.\n",
      "  df[col], _ = df[col].factorize(na_sentinel=-99)\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\2022540250.py:33: FutureWarning: Specifying the specific value to use for `na_sentinel` is deprecated and will be removed in a future version of pandas. Specify `use_na_sentinel=True` to use the sentinel value -1, and `use_na_sentinel=False` to encode NaN values.\n",
      "  df[col], _ = df[col].factorize(na_sentinel=-99)\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\2022540250.py:33: FutureWarning: Specifying the specific value to use for `na_sentinel` is deprecated and will be removed in a future version of pandas. Specify `use_na_sentinel=True` to use the sentinel value -1, and `use_na_sentinel=False` to encode NaN values.\n",
      "  df[col], _ = df[col].factorize(na_sentinel=-99)\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\2022540250.py:33: FutureWarning: Specifying the specific value to use for `na_sentinel` is deprecated and will be removed in a future version of pandas. Specify `use_na_sentinel=True` to use the sentinel value -1, and `use_na_sentinel=False` to encode NaN values.\n",
      "  df[col], _ = df[col].factorize(na_sentinel=-99)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "np.random.seed(2023)\n",
    "\n",
    "trn = pd.read_csv('./data/train_ver2.csv')\n",
    "tst = pd.read_csv('./data/test_ver2.csv')\n",
    "\n",
    "# separates product features\n",
    "prods = trn.columns[24:].tolist()\n",
    "\n",
    "# replace missing values of 0 -> indicating no purchase\n",
    "trn[prods] = trn[prods].fillna(0.0).astype(np.int8)\n",
    "\n",
    "# if the product is all 0, that means all customers did not purchase. We can remove those products\n",
    "no_product = (trn[prods].sum(axis=1) == 0)\n",
    "trn = trn[~no_product]\n",
    "\n",
    "# data that are not in test data are marked as 0.\n",
    "# Then combine train and test data\n",
    "for col in trn.columns[24:]:\n",
    "    tst[col] = 0\n",
    "df = pd.concat([trn, tst], axis=0)\n",
    "\n",
    "# features to use for modeling\n",
    "features = []\n",
    "\n",
    "# perform label encoding\n",
    "categorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi',\n",
    "                    'indext', 'conyuemp', 'canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento']\n",
    "for col in categorical_cols:\n",
    "    df[col], _ = df[col].factorize(na_sentinel=-99)\n",
    "features += categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' 35', ' 23', ' 22', ' 24', ' 65', ' 28', ' 25', ' 26', ' 53',\n",
       "       ' 27', ' 32', ' 37', ' 31', ' 39', ' 63', ' 33', ' 55', ' 42',\n",
       "       ' 58', ' 38', ' 50', ' 30', ' 45', ' 44', ' 36', ' 29', ' 60',\n",
       "       ' 57', ' 67', ' 47', ' NA', ' 34', ' 48', ' 46', ' 54', ' 84',\n",
       "       ' 15', ' 12', '  8', '  6', ' 83', ' 40', ' 77', ' 69', ' 52',\n",
       "       ' 59', ' 43', ' 10', '  9', ' 49', ' 41', ' 51', ' 78', ' 16',\n",
       "       ' 11', ' 73', ' 62', ' 66', ' 17', ' 68', ' 82', ' 95', ' 96',\n",
       "       ' 56', ' 61', ' 79', ' 14', ' 19', ' 13', ' 86', ' 64', ' 20',\n",
       "       ' 72', ' 89', ' 71', '  7', ' 70', ' 74', ' 21', ' 18', ' 75',\n",
       "       '  4', ' 80', ' 81', '  5', ' 76', ' 92', ' 93', ' 85', ' 91',\n",
       "       ' 87', ' 90', ' 94', ' 99', ' 98', ' 88', ' 97', '100', '101',\n",
       "       '106', '103', '  3', '  2', '102', '104', '111', '107', '109',\n",
       "       '105', '112', '115', '110', '116', '108', '113', 37, 81, 43, 30,\n",
       "       41, 67, 59, 46, 36, 47, 69, 39, 44, 38, 34, 42, 31, 40, 48, 54, 51,\n",
       "       33, 62, 35, 50, 45, 11, 57, 55, 80, 70, 60, 32, 61, 13, 29, 28, 49,\n",
       "       63, 88, 58, 52, 79, 53, 85, 77, 82, 10, 56, 89, 68, 72, 66, 64, 71,\n",
       "       74, 75, 27, 22, 23, 76, 78, 65, 21, 26, 19, 83, 90, 12, 14, 86, 93,\n",
       "       18, 84, 87, 17, 97, 102, 16, 73, 91, 15, 25, 94, 24, 95, 20, 96,\n",
       "       92, 110, 104, 106, 105, 99, 98, 101, 107, 9, 113, 100, 103, 109, 2,\n",
       "       112, 3, 111, 8, 7, 6, 108, 115, 116, 5, 4, 114, 117, 164, 118, 127],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['      6', '     35', '     34', '     NA', '     33', '     31',\n",
       "       '     21', '     16', '     27', '      9', '     22', '     13',\n",
       "       '     29', '      8', '     11', '     10', '     28', '     24',\n",
       "       '      7', '     25', '     14', '     12', '     26', '     23',\n",
       "       '      1', '     18', '      4', '      3', '     17', '     32',\n",
       "       '     20', '     15', '     30', '     19', '    157', '     36',\n",
       "       '     40', '     38', '     37', '     39', '      0', '      5',\n",
       "       '     47', '     44', '     42', '     46', '     45', '     43',\n",
       "       '     41', '     57', '     48', '     52', '     49', '     50',\n",
       "       '     56', '     58', '     51', '     55', '     54', '     53',\n",
       "       '     59', '     62', '     61', '     60', '     63', '      2',\n",
       "       '    139', '    165', '    118', '    164', '     94', '    159',\n",
       "       '    143', '    105', '    151', '    162', '    137', '    150',\n",
       "       '    128', '    122', '    156', '    119', '    160', '     79',\n",
       "       '     95', '    132', '    161', '     98', '    127', '     72',\n",
       "       '    155', '    108', '    163', '    102', '    148', '    115',\n",
       "       '    146', '    107', '     81', '    216', '    135', '     92',\n",
       "       '    121', '    198', '    134', '     93', '    140', '    110',\n",
       "       '    120', '    147', '     64', '     77', '     85', '     99',\n",
       "       '     78', '    100', '    113', '    154', '    166', '    124',\n",
       "       '    141', '     66', '    117', '     86', '    193', '     80',\n",
       "       '    144', '     87', '    126', '    158', '    101', '    116',\n",
       "       '    235', '     88', '    145', '    103', '    149', '    109',\n",
       "       '    131', '     97', '    133', '     68', '     84', '    232',\n",
       "       '    125', '    177', '    112', '     96', '     69', '    171',\n",
       "       '    142', '    167', '    104', '     76', '     82', '    152',\n",
       "       '     70', '    138', '    169', '     65', '    129', '    190',\n",
       "       '    114', '    111', '    176', '    153', '     89', '    136',\n",
       "       '     83', '    123', '    187', '    106', '    231', '    189',\n",
       "       '    217', '    172', '    199', '    173', '    174', '    209',\n",
       "       '    180', '    178', '    168', '    130', '    225', '     67',\n",
       "       '     73', '    183', '     74', '    206', '     71', '    184',\n",
       "       '    192', '     91', '     90', '    194', '    182', '    188',\n",
       "       '    213', '    185', '    195', '    186', '    207', '    208',\n",
       "       '     75', '    175', '    201', '    203', '    212', '    215',\n",
       "       '    170', '    228', '    214', '    202', '    196', '    181',\n",
       "       '    211', '    191', '    205', '    200', '    227', '    218',\n",
       "       '    219', '    226', '    179', '    224', '    210', '    242',\n",
       "       '    223', '    237', '    204', '    233', '    220', '    222',\n",
       "       '    241', '    197', '    221', '    229', '    234', '    240',\n",
       "       '    243', '    230', '    238', '    246', '    236', '    244',\n",
       "       '    239', '    245', '-999999', 105, 93, 30, 79, 104, 18, 35, 7,\n",
       "       103, 34, 87, 100, 102, 36, 81, 94, 86, 26, 32, 96, 22, 72, 83, 54,\n",
       "       20, 10, 62, 59, 61, 12, 80, 33, 9, 63, 78, 88, 11, 84, 101, 57, 68,\n",
       "       8, 40, 99, 55, 97, 15, 76, 98, 43, 5, 89, 53, 58, 60, 14, 146, 3,\n",
       "       31, 77, 38, 56, 6, 70, 45, 51, 64, 52, 41, 23, 17, 48, 66, 108, 25,\n",
       "       16, 21, 37, 39, 13, 1, 65, 91, 69, 49, 82, 29, 71, 0, 44, 2, 144,\n",
       "       67, 47, 27, 95, 74, 50, 92, 85, 24, 111, 46, 110, 107, 4, 73, 106,\n",
       "       75, 109, 42, 112, 90, 129, 19, 113, 28, 134, 195, 127, 126, 122,\n",
       "       125, 121, 120, 117, 123, 118, 160, 124, 115, 204, 163, 141, 119,\n",
       "       135, 188, 114, 165, 116, 202, 128, 175, 145, 147, 130, 199, 179,\n",
       "       162, 156, 168, 157, 177, 180, 136, 184, 225, 196, 140, 133, 131,\n",
       "       132, 149, 138, 150, 171, 152, 143, 164, 169, 237, 232, 210, 200,\n",
       "       154, 181, 173, 155, 207, 201, 161, 208, 185, 153, 174, 178, 198,\n",
       "       166, 238, 186, 170, 183, 159, 189, 182, 151, 137, 139, 193, 172,\n",
       "       167, 176, 191, 187, 234, 197, 211, 194, 216, 212, 142, 148, 190,\n",
       "       205, 158, 220, 215, 228, 218, 229, 231, 209, 217, 206, 223, 214,\n",
       "       192, 222, 221, -999999, 226, 233, 241, 235, 236, 227, 219, 224,\n",
       "       240, 203, 213, 239, 230, 242, 243, 244, 246, 245, 247, 248, 249,\n",
       "       250, 251, 252, 253, 254, 255, 256, 257], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['antiguedad'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([87218.1, 35548.74, 122179.11000000002, ..., '  139164.12',\n",
       "       '  100647.45', '   72765.27'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['renta'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0, nan, 3.0, 2.0, '1.0', '1', '3', '3.0', '2.0', 'P', '4', 4.0,\n",
       "       '4.0', '2'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['indrel_1mes'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace singular and missing values ​​of numeric variables with -99 and convert them to integers.\n",
    "df['age'].replace(' NA', -99, inplace=True)\n",
    "df['age'] = df['age'].astype(np.int8)\n",
    "\n",
    "df['antiguedad'].replace('     NA', -99, inplace=True)\n",
    "df['antiguedad'] = df['antiguedad'].astype(np.int8)\n",
    "\n",
    "df['renta'].replace('         NA', -99, inplace=True)\n",
    "df['renta'].fillna(-99, inplace=True)\n",
    "df['renta'] = df['renta'].astype(float).astype(np.int8)\n",
    "\n",
    "df['indrel_1mes'].replace('P', 5, inplace=True)\n",
    "df['indrel_1mes'].fillna(-99, inplace=True)\n",
    "df['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)\n",
    "\n",
    "# Numerical variables to be used for learning are sought in features.\n",
    "features += ['age','antiguedad','renta','ind_nuevo','indrel','indrel_1mes','ind_actividad_cliente']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the baseline model we will use 24 customer features and 4 derivated features from date. Lastely we will use lag value of those 24 customer features to show the previous information.\n",
    "\n",
    "Year and month information are extracted from the variables fecha_alta, which means the date the customer signed the first contract, and ult_fec_cli_1t, which means the date the customer last received the first grade. There are other possible features can be generated reliated with date, such as season or special date like vacation. These can be added after the baseline model\n",
    "\n",
    "In time series data, various derived variables can be created based on the customer's past data. For example, whether the customer's age has changed in the last 3 months (wheter there is birthday or not) can be created as a binary variable, information on products purchased a month ago can be used as a variable, or the average monthly salary for the last 6 months can be calculated.\n",
    "\n",
    "For baseline, we only have -1 lag value but we can also use 2 or 3 lag values to see how customer purchased new products from 2 or 3 months before. For baseline model we only use 1 lag value, but we can add more depends on the performance of the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and month information from two date variables.\n",
    "df['fecha_alta_month'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['fecha_alta_year'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['fecha_alta_month', 'fecha_alta_year']\n",
    "\n",
    "df['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['ult_fec_cli_1t_month', 'ult_fec_cli_1t_year']\n",
    "\n",
    "# All other missing values ​​are replaced with -99.\n",
    "df.fillna(-99, inplace=True)\n",
    "\n",
    "def date_to_int(str_date):\n",
    "    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")] \n",
    "    int_date = (int(Y) - 2015) * 12 + int(M)\n",
    "    return int_date\n",
    "\n",
    "df['int_date'] = df['fecha_dato'].map(date_to_int).astype(np.int8)\n",
    "\n",
    "# Create a lag by copying the data and adding 1 to the int_date date. Add _prev to the variable name.\n",
    "df_lag = df.copy()\n",
    "df_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in df.columns ]\n",
    "df_lag['int_date'] += 1\n",
    "\n",
    "# Merge original data and lag data based on ncodeper and int_date. Since the int_date of the lag data is pushed back by 1, the last month's product information is inserted.\n",
    "df_trn = df.merge(df_lag, on=['ncodpers','int_date'], how='left')\n",
    "\n",
    "# delete after merge for the memory optimization\n",
    "del df, df_lag\n",
    "\n",
    "# Replace with 0 in case product information for the previous month does not exist.\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    df_trn[prev].fillna(0, inplace=True)\n",
    "df_trn.fillna(-99, inplace=True)\n",
    "\n",
    "# Add the lag-1 variable.\n",
    "features += [feature + '_prev' for feature in features]\n",
    "features += [prod + '_prev' for prod in prods]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the baseline model, we should check the results and think of additional featuere engineering and update accordingly. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this prject, a total of 1 year and 6 months of data from 2015-01-28 to 2016-05-28 is provided as training data, and the test data to be predicted is future data from 2016-06-28. Therefore, in the verification process, 2016-05-28, the most recent data in the training data, is separated as verification data, and the rest is used for training.\n",
    "\n",
    "For simplicity, we use data from 2016-01-28 to 2016-05-28 for baseline model training. We will add all dates after we modifying the feature engineering and the model hyperparamter turnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For training, we use date from 2016-01-28 to 2016-05-28 month data.\n",
    "# For validation, we use 2016-05-28 as a validation\n",
    "# Lastly, for test data, we will use 2016-06-28.\n",
    "use_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n",
    "trn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\n",
    "tst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\n",
    "del df_trn\n",
    "\n",
    "# Extract only the number of new purchases from the training data.\n",
    "X = []\n",
    "Y = []\n",
    "for i, prod in enumerate(prods):\n",
    "    prev = prod + '_prev'\n",
    "    prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]\n",
    "    prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n",
    "    X.append(prX)\n",
    "    Y.append(prY)\n",
    "XY = pd.concat(X)\n",
    "Y = np.hstack(Y)\n",
    "XY['y'] = Y\n",
    "\n",
    "vld_date = '2016-05-28'\n",
    "XY_trn = XY[XY['fecha_dato'] != vld_date]\n",
    "XY_vld = XY[XY['fecha_dato'] == vld_date]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "For our baseline model we will use one of the popular machine learning algorithm XGBoost that is widely used in Kaggle as well. It is also great for constructing baseline model as it is fast. Modern libraries like XGBoost come equipped with several speed enhancements, making it possible to train a well-performing model in a short amount of time. XGBoost and similar libraries can also be trained on GPUs (some assembly required), making training even faster on larger datasets.\n",
    "\n",
    "The XGBoost also good for time series forecasting model as it is able to produce reasonable forecasts right out of the box with no hyperparameter tuning. One of the key advantages of XGBoost is its ability to handle missing data and large datasets efficiently. It also has a number of hyperparameters that can be tuned to improve model performance, including the learning rate, depth of the trees, and regularization parameters\n",
    "\n",
    "The caveat is that most people tend to spend too much time finding the optimal parameters for their models. This task is called hyperparameter tuning. Finding good parameters can yield significant performance improvements. However, in terms of efficiency versus time investment, it is important to spend more time on feature engineering rather than hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:58:39] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-mlogloss:2.70583\teval-mlogloss:2.76067\n",
      "[1]\ttrain-mlogloss:2.44127\teval-mlogloss:2.49695\n",
      "[2]\ttrain-mlogloss:2.26188\teval-mlogloss:2.31725\n",
      "[3]\ttrain-mlogloss:2.12551\teval-mlogloss:2.18293\n",
      "[4]\ttrain-mlogloss:2.01400\teval-mlogloss:2.07096\n",
      "[5]\ttrain-mlogloss:1.91945\teval-mlogloss:1.97631\n",
      "[6]\ttrain-mlogloss:1.84273\teval-mlogloss:1.89985\n",
      "[7]\ttrain-mlogloss:1.77555\teval-mlogloss:1.83276\n",
      "[8]\ttrain-mlogloss:1.71464\teval-mlogloss:1.77159\n",
      "[9]\ttrain-mlogloss:1.66296\teval-mlogloss:1.71996\n",
      "[10]\ttrain-mlogloss:1.61705\teval-mlogloss:1.67376\n",
      "[11]\ttrain-mlogloss:1.57471\teval-mlogloss:1.63104\n",
      "[12]\ttrain-mlogloss:1.53632\teval-mlogloss:1.59223\n",
      "[13]\ttrain-mlogloss:1.50267\teval-mlogloss:1.55879\n",
      "[14]\ttrain-mlogloss:1.47280\teval-mlogloss:1.52845\n",
      "[15]\ttrain-mlogloss:1.44417\teval-mlogloss:1.49898\n",
      "[16]\ttrain-mlogloss:1.41890\teval-mlogloss:1.47336\n",
      "[17]\ttrain-mlogloss:1.39626\teval-mlogloss:1.45012\n",
      "[18]\ttrain-mlogloss:1.37488\teval-mlogloss:1.42830\n",
      "[19]\ttrain-mlogloss:1.35557\teval-mlogloss:1.40856\n",
      "[20]\ttrain-mlogloss:1.33801\teval-mlogloss:1.39030\n",
      "[21]\ttrain-mlogloss:1.32091\teval-mlogloss:1.37270\n",
      "[22]\ttrain-mlogloss:1.30571\teval-mlogloss:1.35720\n",
      "[23]\ttrain-mlogloss:1.29169\teval-mlogloss:1.34279\n",
      "[24]\ttrain-mlogloss:1.27889\teval-mlogloss:1.32976\n",
      "[25]\ttrain-mlogloss:1.26655\teval-mlogloss:1.31694\n",
      "[26]\ttrain-mlogloss:1.25533\teval-mlogloss:1.30528\n",
      "[27]\ttrain-mlogloss:1.24482\teval-mlogloss:1.29427\n",
      "[28]\ttrain-mlogloss:1.23512\teval-mlogloss:1.28443\n",
      "[29]\ttrain-mlogloss:1.22649\teval-mlogloss:1.27545\n",
      "[30]\ttrain-mlogloss:1.21846\teval-mlogloss:1.26718\n",
      "[31]\ttrain-mlogloss:1.21050\teval-mlogloss:1.25877\n",
      "[32]\ttrain-mlogloss:1.20299\teval-mlogloss:1.25098\n",
      "[33]\ttrain-mlogloss:1.19628\teval-mlogloss:1.24384\n",
      "[34]\ttrain-mlogloss:1.18984\teval-mlogloss:1.23690\n",
      "[35]\ttrain-mlogloss:1.18376\teval-mlogloss:1.23041\n",
      "[36]\ttrain-mlogloss:1.17801\teval-mlogloss:1.22420\n",
      "[37]\ttrain-mlogloss:1.17261\teval-mlogloss:1.21840\n",
      "[38]\ttrain-mlogloss:1.16761\teval-mlogloss:1.21329\n",
      "[39]\ttrain-mlogloss:1.16286\teval-mlogloss:1.20845\n",
      "[40]\ttrain-mlogloss:1.15849\teval-mlogloss:1.20377\n",
      "[41]\ttrain-mlogloss:1.15425\teval-mlogloss:1.19941\n",
      "[42]\ttrain-mlogloss:1.15037\teval-mlogloss:1.19545\n",
      "[43]\ttrain-mlogloss:1.14660\teval-mlogloss:1.19152\n",
      "[44]\ttrain-mlogloss:1.14296\teval-mlogloss:1.18769\n",
      "[45]\ttrain-mlogloss:1.13968\teval-mlogloss:1.18434\n",
      "[46]\ttrain-mlogloss:1.13649\teval-mlogloss:1.18109\n",
      "[47]\ttrain-mlogloss:1.13334\teval-mlogloss:1.17791\n",
      "[48]\ttrain-mlogloss:1.13040\teval-mlogloss:1.17493\n",
      "[49]\ttrain-mlogloss:1.12760\teval-mlogloss:1.17212\n",
      "[50]\ttrain-mlogloss:1.12495\teval-mlogloss:1.16949\n",
      "[51]\ttrain-mlogloss:1.12245\teval-mlogloss:1.16694\n",
      "[52]\ttrain-mlogloss:1.12013\teval-mlogloss:1.16468\n",
      "[53]\ttrain-mlogloss:1.11780\teval-mlogloss:1.16223\n",
      "[54]\ttrain-mlogloss:1.11561\teval-mlogloss:1.16004\n",
      "[55]\ttrain-mlogloss:1.11350\teval-mlogloss:1.15794\n",
      "[56]\ttrain-mlogloss:1.11158\teval-mlogloss:1.15598\n",
      "[57]\ttrain-mlogloss:1.10974\teval-mlogloss:1.15423\n",
      "[58]\ttrain-mlogloss:1.10796\teval-mlogloss:1.15244\n",
      "[59]\ttrain-mlogloss:1.10627\teval-mlogloss:1.15080\n",
      "[60]\ttrain-mlogloss:1.10462\teval-mlogloss:1.14919\n",
      "[61]\ttrain-mlogloss:1.10315\teval-mlogloss:1.14773\n",
      "[62]\ttrain-mlogloss:1.10171\teval-mlogloss:1.14634\n",
      "[63]\ttrain-mlogloss:1.10025\teval-mlogloss:1.14490\n",
      "[64]\ttrain-mlogloss:1.09892\teval-mlogloss:1.14358\n",
      "[65]\ttrain-mlogloss:1.09764\teval-mlogloss:1.14234\n",
      "[66]\ttrain-mlogloss:1.09639\teval-mlogloss:1.14116\n",
      "[67]\ttrain-mlogloss:1.09518\teval-mlogloss:1.13996\n",
      "[68]\ttrain-mlogloss:1.09413\teval-mlogloss:1.13895\n",
      "[69]\ttrain-mlogloss:1.09298\teval-mlogloss:1.13791\n",
      "[70]\ttrain-mlogloss:1.09188\teval-mlogloss:1.13693\n",
      "[71]\ttrain-mlogloss:1.09090\teval-mlogloss:1.13596\n",
      "[72]\ttrain-mlogloss:1.08988\teval-mlogloss:1.13504\n",
      "[73]\ttrain-mlogloss:1.08890\teval-mlogloss:1.13419\n",
      "[74]\ttrain-mlogloss:1.08791\teval-mlogloss:1.13330\n",
      "[75]\ttrain-mlogloss:1.08699\teval-mlogloss:1.13248\n",
      "[76]\ttrain-mlogloss:1.08604\teval-mlogloss:1.13166\n",
      "[77]\ttrain-mlogloss:1.08513\teval-mlogloss:1.13092\n",
      "[78]\ttrain-mlogloss:1.08431\teval-mlogloss:1.13024\n",
      "[79]\ttrain-mlogloss:1.08354\teval-mlogloss:1.12956\n",
      "[80]\ttrain-mlogloss:1.08282\teval-mlogloss:1.12891\n",
      "[81]\ttrain-mlogloss:1.08207\teval-mlogloss:1.12828\n",
      "[82]\ttrain-mlogloss:1.08131\teval-mlogloss:1.12767\n",
      "[83]\ttrain-mlogloss:1.08060\teval-mlogloss:1.12713\n",
      "[84]\ttrain-mlogloss:1.07990\teval-mlogloss:1.12657\n",
      "[85]\ttrain-mlogloss:1.07924\teval-mlogloss:1.12603\n",
      "[86]\ttrain-mlogloss:1.07860\teval-mlogloss:1.12559\n",
      "[87]\ttrain-mlogloss:1.07794\teval-mlogloss:1.12517\n",
      "[88]\ttrain-mlogloss:1.07724\teval-mlogloss:1.12474\n",
      "[89]\ttrain-mlogloss:1.07658\teval-mlogloss:1.12428\n",
      "[90]\ttrain-mlogloss:1.07600\teval-mlogloss:1.12382\n",
      "[91]\ttrain-mlogloss:1.07533\teval-mlogloss:1.12337\n",
      "[92]\ttrain-mlogloss:1.07467\teval-mlogloss:1.12287\n",
      "[93]\ttrain-mlogloss:1.07418\teval-mlogloss:1.12252\n",
      "[94]\ttrain-mlogloss:1.07353\teval-mlogloss:1.12210\n",
      "[95]\ttrain-mlogloss:1.07299\teval-mlogloss:1.12179\n",
      "[96]\ttrain-mlogloss:1.07248\teval-mlogloss:1.12147\n",
      "[97]\ttrain-mlogloss:1.07189\teval-mlogloss:1.12104\n",
      "[98]\ttrain-mlogloss:1.07142\teval-mlogloss:1.12073\n",
      "[99]\ttrain-mlogloss:1.07091\teval-mlogloss:1.12040\n",
      "[100]\ttrain-mlogloss:1.07042\teval-mlogloss:1.12011\n",
      "[101]\ttrain-mlogloss:1.06989\teval-mlogloss:1.11977\n",
      "[102]\ttrain-mlogloss:1.06933\teval-mlogloss:1.11947\n",
      "[103]\ttrain-mlogloss:1.06889\teval-mlogloss:1.11918\n",
      "[104]\ttrain-mlogloss:1.06840\teval-mlogloss:1.11886\n",
      "[105]\ttrain-mlogloss:1.06797\teval-mlogloss:1.11871\n",
      "[106]\ttrain-mlogloss:1.06754\teval-mlogloss:1.11844\n",
      "[107]\ttrain-mlogloss:1.06714\teval-mlogloss:1.11822\n",
      "[108]\ttrain-mlogloss:1.06676\teval-mlogloss:1.11799\n",
      "[109]\ttrain-mlogloss:1.06620\teval-mlogloss:1.11768\n",
      "[110]\ttrain-mlogloss:1.06566\teval-mlogloss:1.11742\n",
      "[111]\ttrain-mlogloss:1.06532\teval-mlogloss:1.11721\n",
      "[112]\ttrain-mlogloss:1.06484\teval-mlogloss:1.11702\n",
      "[113]\ttrain-mlogloss:1.06447\teval-mlogloss:1.11684\n",
      "[114]\ttrain-mlogloss:1.06403\teval-mlogloss:1.11658\n",
      "[115]\ttrain-mlogloss:1.06366\teval-mlogloss:1.11640\n",
      "[116]\ttrain-mlogloss:1.06335\teval-mlogloss:1.11618\n",
      "[117]\ttrain-mlogloss:1.06294\teval-mlogloss:1.11597\n",
      "[118]\ttrain-mlogloss:1.06264\teval-mlogloss:1.11581\n",
      "[119]\ttrain-mlogloss:1.06226\teval-mlogloss:1.11564\n",
      "[120]\ttrain-mlogloss:1.06193\teval-mlogloss:1.11551\n",
      "[121]\ttrain-mlogloss:1.06157\teval-mlogloss:1.11529\n",
      "[122]\ttrain-mlogloss:1.06120\teval-mlogloss:1.11508\n",
      "[123]\ttrain-mlogloss:1.06075\teval-mlogloss:1.11486\n",
      "[124]\ttrain-mlogloss:1.06035\teval-mlogloss:1.11465\n",
      "[125]\ttrain-mlogloss:1.05999\teval-mlogloss:1.11449\n",
      "[126]\ttrain-mlogloss:1.05957\teval-mlogloss:1.11430\n",
      "[127]\ttrain-mlogloss:1.05924\teval-mlogloss:1.11417\n",
      "[128]\ttrain-mlogloss:1.05893\teval-mlogloss:1.11399\n",
      "[129]\ttrain-mlogloss:1.05855\teval-mlogloss:1.11383\n",
      "[130]\ttrain-mlogloss:1.05826\teval-mlogloss:1.11369\n",
      "[131]\ttrain-mlogloss:1.05784\teval-mlogloss:1.11351\n",
      "[132]\ttrain-mlogloss:1.05744\teval-mlogloss:1.11331\n",
      "[133]\ttrain-mlogloss:1.05711\teval-mlogloss:1.11319\n",
      "[134]\ttrain-mlogloss:1.05676\teval-mlogloss:1.11306\n",
      "[135]\ttrain-mlogloss:1.05636\teval-mlogloss:1.11290\n",
      "[136]\ttrain-mlogloss:1.05599\teval-mlogloss:1.11275\n",
      "[137]\ttrain-mlogloss:1.05560\teval-mlogloss:1.11255\n",
      "[138]\ttrain-mlogloss:1.05520\teval-mlogloss:1.11245\n",
      "[139]\ttrain-mlogloss:1.05488\teval-mlogloss:1.11231\n",
      "[140]\ttrain-mlogloss:1.05449\teval-mlogloss:1.11213\n",
      "[141]\ttrain-mlogloss:1.05415\teval-mlogloss:1.11204\n",
      "[142]\ttrain-mlogloss:1.05380\teval-mlogloss:1.11189\n",
      "[143]\ttrain-mlogloss:1.05345\teval-mlogloss:1.11175\n",
      "[144]\ttrain-mlogloss:1.05314\teval-mlogloss:1.11163\n",
      "[145]\ttrain-mlogloss:1.05277\teval-mlogloss:1.11151\n",
      "[146]\ttrain-mlogloss:1.05242\teval-mlogloss:1.11138\n",
      "[147]\ttrain-mlogloss:1.05209\teval-mlogloss:1.11125\n",
      "[148]\ttrain-mlogloss:1.05172\teval-mlogloss:1.11114\n",
      "[149]\ttrain-mlogloss:1.05140\teval-mlogloss:1.11100\n",
      "[150]\ttrain-mlogloss:1.05108\teval-mlogloss:1.11091\n",
      "[151]\ttrain-mlogloss:1.05073\teval-mlogloss:1.11079\n",
      "[152]\ttrain-mlogloss:1.05038\teval-mlogloss:1.11069\n",
      "[153]\ttrain-mlogloss:1.04991\teval-mlogloss:1.11052\n",
      "[154]\ttrain-mlogloss:1.04955\teval-mlogloss:1.11040\n",
      "[155]\ttrain-mlogloss:1.04920\teval-mlogloss:1.11028\n",
      "[156]\ttrain-mlogloss:1.04888\teval-mlogloss:1.11017\n",
      "[157]\ttrain-mlogloss:1.04861\teval-mlogloss:1.11007\n",
      "[158]\ttrain-mlogloss:1.04826\teval-mlogloss:1.10999\n",
      "[159]\ttrain-mlogloss:1.04795\teval-mlogloss:1.10987\n",
      "[160]\ttrain-mlogloss:1.04762\teval-mlogloss:1.10982\n",
      "[161]\ttrain-mlogloss:1.04721\teval-mlogloss:1.10972\n",
      "[162]\ttrain-mlogloss:1.04689\teval-mlogloss:1.10963\n",
      "[163]\ttrain-mlogloss:1.04659\teval-mlogloss:1.10952\n",
      "[164]\ttrain-mlogloss:1.04620\teval-mlogloss:1.10940\n",
      "[165]\ttrain-mlogloss:1.04584\teval-mlogloss:1.10933\n",
      "[166]\ttrain-mlogloss:1.04547\teval-mlogloss:1.10925\n",
      "[167]\ttrain-mlogloss:1.04511\teval-mlogloss:1.10911\n",
      "[168]\ttrain-mlogloss:1.04474\teval-mlogloss:1.10903\n",
      "[169]\ttrain-mlogloss:1.04440\teval-mlogloss:1.10896\n",
      "[170]\ttrain-mlogloss:1.04397\teval-mlogloss:1.10888\n",
      "[171]\ttrain-mlogloss:1.04367\teval-mlogloss:1.10877\n",
      "[172]\ttrain-mlogloss:1.04329\teval-mlogloss:1.10863\n",
      "[173]\ttrain-mlogloss:1.04303\teval-mlogloss:1.10854\n",
      "[174]\ttrain-mlogloss:1.04266\teval-mlogloss:1.10846\n",
      "[175]\ttrain-mlogloss:1.04233\teval-mlogloss:1.10835\n",
      "[176]\ttrain-mlogloss:1.04194\teval-mlogloss:1.10826\n",
      "[177]\ttrain-mlogloss:1.04149\teval-mlogloss:1.10801\n",
      "[178]\ttrain-mlogloss:1.04108\teval-mlogloss:1.10790\n",
      "[179]\ttrain-mlogloss:1.04083\teval-mlogloss:1.10792\n",
      "[180]\ttrain-mlogloss:1.04048\teval-mlogloss:1.10779\n",
      "[181]\ttrain-mlogloss:1.04022\teval-mlogloss:1.10771\n",
      "[182]\ttrain-mlogloss:1.03993\teval-mlogloss:1.10764\n",
      "[183]\ttrain-mlogloss:1.03964\teval-mlogloss:1.10762\n",
      "[184]\ttrain-mlogloss:1.03934\teval-mlogloss:1.10745\n",
      "[185]\ttrain-mlogloss:1.03888\teval-mlogloss:1.10720\n",
      "[186]\ttrain-mlogloss:1.03856\teval-mlogloss:1.10712\n",
      "[187]\ttrain-mlogloss:1.03824\teval-mlogloss:1.10709\n",
      "[188]\ttrain-mlogloss:1.03786\teval-mlogloss:1.10691\n",
      "[189]\ttrain-mlogloss:1.03757\teval-mlogloss:1.10683\n",
      "[190]\ttrain-mlogloss:1.03730\teval-mlogloss:1.10672\n",
      "[191]\ttrain-mlogloss:1.03691\teval-mlogloss:1.10662\n",
      "[192]\ttrain-mlogloss:1.03651\teval-mlogloss:1.10659\n",
      "[193]\ttrain-mlogloss:1.03617\teval-mlogloss:1.10647\n",
      "[194]\ttrain-mlogloss:1.03582\teval-mlogloss:1.10636\n",
      "[195]\ttrain-mlogloss:1.03539\teval-mlogloss:1.10624\n",
      "[196]\ttrain-mlogloss:1.03494\teval-mlogloss:1.10617\n",
      "[197]\ttrain-mlogloss:1.03461\teval-mlogloss:1.10606\n",
      "[198]\ttrain-mlogloss:1.03424\teval-mlogloss:1.10589\n",
      "[199]\ttrain-mlogloss:1.03383\teval-mlogloss:1.10574\n",
      "[200]\ttrain-mlogloss:1.03347\teval-mlogloss:1.10568\n",
      "[201]\ttrain-mlogloss:1.03308\teval-mlogloss:1.10557\n",
      "[202]\ttrain-mlogloss:1.03276\teval-mlogloss:1.10550\n",
      "[203]\ttrain-mlogloss:1.03242\teval-mlogloss:1.10538\n",
      "[204]\ttrain-mlogloss:1.03200\teval-mlogloss:1.10525\n",
      "[205]\ttrain-mlogloss:1.03173\teval-mlogloss:1.10521\n",
      "[206]\ttrain-mlogloss:1.03137\teval-mlogloss:1.10511\n",
      "[207]\ttrain-mlogloss:1.03109\teval-mlogloss:1.10507\n",
      "[208]\ttrain-mlogloss:1.03075\teval-mlogloss:1.10496\n",
      "[209]\ttrain-mlogloss:1.03032\teval-mlogloss:1.10479\n",
      "[210]\ttrain-mlogloss:1.02993\teval-mlogloss:1.10469\n",
      "[211]\ttrain-mlogloss:1.02963\teval-mlogloss:1.10462\n",
      "[212]\ttrain-mlogloss:1.02928\teval-mlogloss:1.10452\n",
      "[213]\ttrain-mlogloss:1.02898\teval-mlogloss:1.10442\n",
      "[214]\ttrain-mlogloss:1.02861\teval-mlogloss:1.10435\n",
      "[215]\ttrain-mlogloss:1.02832\teval-mlogloss:1.10428\n",
      "[216]\ttrain-mlogloss:1.02807\teval-mlogloss:1.10424\n",
      "[217]\ttrain-mlogloss:1.02770\teval-mlogloss:1.10415\n",
      "[218]\ttrain-mlogloss:1.02738\teval-mlogloss:1.10401\n",
      "[219]\ttrain-mlogloss:1.02711\teval-mlogloss:1.10399\n",
      "[220]\ttrain-mlogloss:1.02675\teval-mlogloss:1.10390\n",
      "[221]\ttrain-mlogloss:1.02645\teval-mlogloss:1.10385\n",
      "[222]\ttrain-mlogloss:1.02615\teval-mlogloss:1.10375\n",
      "[223]\ttrain-mlogloss:1.02582\teval-mlogloss:1.10368\n",
      "[224]\ttrain-mlogloss:1.02551\teval-mlogloss:1.10363\n",
      "[225]\ttrain-mlogloss:1.02521\teval-mlogloss:1.10355\n",
      "[226]\ttrain-mlogloss:1.02491\teval-mlogloss:1.10346\n",
      "[227]\ttrain-mlogloss:1.02463\teval-mlogloss:1.10338\n",
      "[228]\ttrain-mlogloss:1.02433\teval-mlogloss:1.10329\n",
      "[229]\ttrain-mlogloss:1.02404\teval-mlogloss:1.10325\n",
      "[230]\ttrain-mlogloss:1.02373\teval-mlogloss:1.10315\n",
      "[231]\ttrain-mlogloss:1.02349\teval-mlogloss:1.10311\n",
      "[232]\ttrain-mlogloss:1.02321\teval-mlogloss:1.10306\n",
      "[233]\ttrain-mlogloss:1.02283\teval-mlogloss:1.10297\n",
      "[234]\ttrain-mlogloss:1.02254\teval-mlogloss:1.10289\n",
      "[235]\ttrain-mlogloss:1.02220\teval-mlogloss:1.10284\n",
      "[236]\ttrain-mlogloss:1.02184\teval-mlogloss:1.10276\n",
      "[237]\ttrain-mlogloss:1.02144\teval-mlogloss:1.10255\n",
      "[238]\ttrain-mlogloss:1.02114\teval-mlogloss:1.10246\n",
      "[239]\ttrain-mlogloss:1.02072\teval-mlogloss:1.10232\n",
      "[240]\ttrain-mlogloss:1.02035\teval-mlogloss:1.10221\n",
      "[241]\ttrain-mlogloss:1.02004\teval-mlogloss:1.10206\n",
      "[242]\ttrain-mlogloss:1.01974\teval-mlogloss:1.10197\n",
      "[243]\ttrain-mlogloss:1.01946\teval-mlogloss:1.10191\n",
      "[244]\ttrain-mlogloss:1.01914\teval-mlogloss:1.10185\n",
      "[245]\ttrain-mlogloss:1.01885\teval-mlogloss:1.10181\n",
      "[246]\ttrain-mlogloss:1.01854\teval-mlogloss:1.10178\n",
      "[247]\ttrain-mlogloss:1.01824\teval-mlogloss:1.10170\n",
      "[248]\ttrain-mlogloss:1.01798\teval-mlogloss:1.10156\n",
      "[249]\ttrain-mlogloss:1.01769\teval-mlogloss:1.10151\n",
      "[250]\ttrain-mlogloss:1.01732\teval-mlogloss:1.10143\n",
      "[251]\ttrain-mlogloss:1.01699\teval-mlogloss:1.10137\n",
      "[252]\ttrain-mlogloss:1.01671\teval-mlogloss:1.10129\n",
      "[253]\ttrain-mlogloss:1.01646\teval-mlogloss:1.10125\n",
      "[254]\ttrain-mlogloss:1.01611\teval-mlogloss:1.10114\n",
      "[255]\ttrain-mlogloss:1.01582\teval-mlogloss:1.10105\n",
      "[256]\ttrain-mlogloss:1.01545\teval-mlogloss:1.10097\n",
      "[257]\ttrain-mlogloss:1.01500\teval-mlogloss:1.10082\n",
      "[258]\ttrain-mlogloss:1.01468\teval-mlogloss:1.10076\n",
      "[259]\ttrain-mlogloss:1.01441\teval-mlogloss:1.10070\n",
      "[260]\ttrain-mlogloss:1.01408\teval-mlogloss:1.10063\n",
      "[261]\ttrain-mlogloss:1.01380\teval-mlogloss:1.10054\n",
      "[262]\ttrain-mlogloss:1.01351\teval-mlogloss:1.10046\n",
      "[263]\ttrain-mlogloss:1.01324\teval-mlogloss:1.10039\n",
      "[264]\ttrain-mlogloss:1.01294\teval-mlogloss:1.10031\n",
      "[265]\ttrain-mlogloss:1.01263\teval-mlogloss:1.10018\n",
      "[266]\ttrain-mlogloss:1.01227\teval-mlogloss:1.10009\n",
      "[267]\ttrain-mlogloss:1.01204\teval-mlogloss:1.10009\n",
      "[268]\ttrain-mlogloss:1.01168\teval-mlogloss:1.10006\n",
      "[269]\ttrain-mlogloss:1.01145\teval-mlogloss:1.10001\n",
      "[270]\ttrain-mlogloss:1.01102\teval-mlogloss:1.09985\n",
      "[271]\ttrain-mlogloss:1.01079\teval-mlogloss:1.09981\n",
      "[272]\ttrain-mlogloss:1.01046\teval-mlogloss:1.09975\n",
      "[273]\ttrain-mlogloss:1.01014\teval-mlogloss:1.09969\n",
      "[274]\ttrain-mlogloss:1.00988\teval-mlogloss:1.09965\n",
      "[275]\ttrain-mlogloss:1.00953\teval-mlogloss:1.09960\n",
      "[276]\ttrain-mlogloss:1.00920\teval-mlogloss:1.09959\n",
      "[277]\ttrain-mlogloss:1.00887\teval-mlogloss:1.09955\n",
      "[278]\ttrain-mlogloss:1.00864\teval-mlogloss:1.09952\n",
      "[279]\ttrain-mlogloss:1.00836\teval-mlogloss:1.09950\n",
      "[280]\ttrain-mlogloss:1.00810\teval-mlogloss:1.09950\n",
      "[281]\ttrain-mlogloss:1.00781\teval-mlogloss:1.09942\n",
      "[282]\ttrain-mlogloss:1.00750\teval-mlogloss:1.09933\n",
      "[283]\ttrain-mlogloss:1.00722\teval-mlogloss:1.09930\n",
      "[284]\ttrain-mlogloss:1.00686\teval-mlogloss:1.09919\n",
      "[285]\ttrain-mlogloss:1.00650\teval-mlogloss:1.09908\n",
      "[286]\ttrain-mlogloss:1.00619\teval-mlogloss:1.09899\n",
      "[287]\ttrain-mlogloss:1.00587\teval-mlogloss:1.09886\n",
      "[288]\ttrain-mlogloss:1.00554\teval-mlogloss:1.09881\n",
      "[289]\ttrain-mlogloss:1.00521\teval-mlogloss:1.09875\n",
      "[290]\ttrain-mlogloss:1.00492\teval-mlogloss:1.09862\n",
      "[291]\ttrain-mlogloss:1.00463\teval-mlogloss:1.09857\n",
      "[292]\ttrain-mlogloss:1.00431\teval-mlogloss:1.09854\n",
      "[293]\ttrain-mlogloss:1.00402\teval-mlogloss:1.09848\n",
      "[294]\ttrain-mlogloss:1.00370\teval-mlogloss:1.09836\n",
      "[295]\ttrain-mlogloss:1.00334\teval-mlogloss:1.09827\n",
      "[296]\ttrain-mlogloss:1.00308\teval-mlogloss:1.09820\n",
      "[297]\ttrain-mlogloss:1.00282\teval-mlogloss:1.09814\n",
      "[298]\ttrain-mlogloss:1.00252\teval-mlogloss:1.09806\n",
      "[299]\ttrain-mlogloss:1.00221\teval-mlogloss:1.09800\n",
      "[300]\ttrain-mlogloss:1.00195\teval-mlogloss:1.09798\n",
      "[301]\ttrain-mlogloss:1.00158\teval-mlogloss:1.09788\n",
      "[302]\ttrain-mlogloss:1.00127\teval-mlogloss:1.09776\n",
      "[303]\ttrain-mlogloss:1.00106\teval-mlogloss:1.09771\n",
      "[304]\ttrain-mlogloss:1.00080\teval-mlogloss:1.09768\n",
      "[305]\ttrain-mlogloss:1.00056\teval-mlogloss:1.09764\n",
      "[306]\ttrain-mlogloss:1.00026\teval-mlogloss:1.09760\n",
      "[307]\ttrain-mlogloss:0.99994\teval-mlogloss:1.09753\n",
      "[308]\ttrain-mlogloss:0.99959\teval-mlogloss:1.09744\n",
      "[309]\ttrain-mlogloss:0.99928\teval-mlogloss:1.09731\n",
      "[310]\ttrain-mlogloss:0.99904\teval-mlogloss:1.09724\n",
      "[311]\ttrain-mlogloss:0.99872\teval-mlogloss:1.09719\n",
      "[312]\ttrain-mlogloss:0.99848\teval-mlogloss:1.09716\n",
      "[313]\ttrain-mlogloss:0.99823\teval-mlogloss:1.09711\n",
      "[314]\ttrain-mlogloss:0.99799\teval-mlogloss:1.09706\n",
      "[315]\ttrain-mlogloss:0.99771\teval-mlogloss:1.09698\n",
      "[316]\ttrain-mlogloss:0.99747\teval-mlogloss:1.09690\n",
      "[317]\ttrain-mlogloss:0.99720\teval-mlogloss:1.09684\n",
      "[318]\ttrain-mlogloss:0.99698\teval-mlogloss:1.09682\n",
      "[319]\ttrain-mlogloss:0.99667\teval-mlogloss:1.09671\n",
      "[320]\ttrain-mlogloss:0.99638\teval-mlogloss:1.09664\n",
      "[321]\ttrain-mlogloss:0.99617\teval-mlogloss:1.09661\n",
      "[322]\ttrain-mlogloss:0.99586\teval-mlogloss:1.09646\n",
      "[323]\ttrain-mlogloss:0.99565\teval-mlogloss:1.09644\n",
      "[324]\ttrain-mlogloss:0.99545\teval-mlogloss:1.09640\n",
      "[325]\ttrain-mlogloss:0.99516\teval-mlogloss:1.09638\n",
      "[326]\ttrain-mlogloss:0.99478\teval-mlogloss:1.09626\n",
      "[327]\ttrain-mlogloss:0.99444\teval-mlogloss:1.09615\n",
      "[328]\ttrain-mlogloss:0.99414\teval-mlogloss:1.09609\n",
      "[329]\ttrain-mlogloss:0.99384\teval-mlogloss:1.09603\n",
      "[330]\ttrain-mlogloss:0.99348\teval-mlogloss:1.09597\n",
      "[331]\ttrain-mlogloss:0.99316\teval-mlogloss:1.09594\n",
      "[332]\ttrain-mlogloss:0.99292\teval-mlogloss:1.09592\n",
      "[333]\ttrain-mlogloss:0.99266\teval-mlogloss:1.09589\n",
      "[334]\ttrain-mlogloss:0.99244\teval-mlogloss:1.09582\n",
      "[335]\ttrain-mlogloss:0.99219\teval-mlogloss:1.09576\n",
      "[336]\ttrain-mlogloss:0.99193\teval-mlogloss:1.09567\n",
      "[337]\ttrain-mlogloss:0.99160\teval-mlogloss:1.09558\n",
      "[338]\ttrain-mlogloss:0.99131\teval-mlogloss:1.09540\n",
      "[339]\ttrain-mlogloss:0.99102\teval-mlogloss:1.09541\n",
      "[340]\ttrain-mlogloss:0.99074\teval-mlogloss:1.09534\n",
      "[341]\ttrain-mlogloss:0.99049\teval-mlogloss:1.09531\n",
      "[342]\ttrain-mlogloss:0.99028\teval-mlogloss:1.09522\n",
      "[343]\ttrain-mlogloss:0.99006\teval-mlogloss:1.09519\n",
      "[344]\ttrain-mlogloss:0.98977\teval-mlogloss:1.09514\n",
      "[345]\ttrain-mlogloss:0.98946\teval-mlogloss:1.09506\n",
      "[346]\ttrain-mlogloss:0.98914\teval-mlogloss:1.09499\n",
      "[347]\ttrain-mlogloss:0.98887\teval-mlogloss:1.09492\n",
      "[348]\ttrain-mlogloss:0.98866\teval-mlogloss:1.09485\n",
      "[349]\ttrain-mlogloss:0.98835\teval-mlogloss:1.09481\n",
      "[350]\ttrain-mlogloss:0.98806\teval-mlogloss:1.09474\n",
      "[351]\ttrain-mlogloss:0.98777\teval-mlogloss:1.09474\n",
      "[352]\ttrain-mlogloss:0.98751\teval-mlogloss:1.09472\n",
      "[353]\ttrain-mlogloss:0.98728\teval-mlogloss:1.09469\n",
      "[354]\ttrain-mlogloss:0.98705\teval-mlogloss:1.09463\n",
      "[355]\ttrain-mlogloss:0.98680\teval-mlogloss:1.09458\n",
      "[356]\ttrain-mlogloss:0.98650\teval-mlogloss:1.09454\n",
      "[357]\ttrain-mlogloss:0.98621\teval-mlogloss:1.09445\n",
      "[358]\ttrain-mlogloss:0.98589\teval-mlogloss:1.09440\n",
      "[359]\ttrain-mlogloss:0.98560\teval-mlogloss:1.09436\n",
      "[360]\ttrain-mlogloss:0.98535\teval-mlogloss:1.09431\n",
      "[361]\ttrain-mlogloss:0.98507\teval-mlogloss:1.09427\n",
      "[362]\ttrain-mlogloss:0.98481\teval-mlogloss:1.09423\n",
      "[363]\ttrain-mlogloss:0.98457\teval-mlogloss:1.09418\n",
      "[364]\ttrain-mlogloss:0.98435\teval-mlogloss:1.09416\n",
      "[365]\ttrain-mlogloss:0.98408\teval-mlogloss:1.09415\n",
      "[366]\ttrain-mlogloss:0.98389\teval-mlogloss:1.09414\n",
      "[367]\ttrain-mlogloss:0.98358\teval-mlogloss:1.09405\n",
      "[368]\ttrain-mlogloss:0.98334\teval-mlogloss:1.09402\n",
      "[369]\ttrain-mlogloss:0.98307\teval-mlogloss:1.09397\n",
      "[370]\ttrain-mlogloss:0.98286\teval-mlogloss:1.09394\n",
      "[371]\ttrain-mlogloss:0.98265\teval-mlogloss:1.09392\n",
      "[372]\ttrain-mlogloss:0.98245\teval-mlogloss:1.09388\n",
      "[373]\ttrain-mlogloss:0.98213\teval-mlogloss:1.09385\n",
      "[374]\ttrain-mlogloss:0.98191\teval-mlogloss:1.09380\n",
      "[375]\ttrain-mlogloss:0.98165\teval-mlogloss:1.09371\n",
      "[376]\ttrain-mlogloss:0.98140\teval-mlogloss:1.09364\n",
      "[377]\ttrain-mlogloss:0.98112\teval-mlogloss:1.09355\n",
      "[378]\ttrain-mlogloss:0.98086\teval-mlogloss:1.09350\n",
      "[379]\ttrain-mlogloss:0.98060\teval-mlogloss:1.09346\n",
      "[380]\ttrain-mlogloss:0.98038\teval-mlogloss:1.09339\n",
      "[381]\ttrain-mlogloss:0.98015\teval-mlogloss:1.09329\n",
      "[382]\ttrain-mlogloss:0.97990\teval-mlogloss:1.09319\n",
      "[383]\ttrain-mlogloss:0.97963\teval-mlogloss:1.09311\n",
      "[384]\ttrain-mlogloss:0.97940\teval-mlogloss:1.09309\n",
      "[385]\ttrain-mlogloss:0.97917\teval-mlogloss:1.09305\n",
      "[386]\ttrain-mlogloss:0.97894\teval-mlogloss:1.09300\n",
      "[387]\ttrain-mlogloss:0.97873\teval-mlogloss:1.09299\n",
      "[388]\ttrain-mlogloss:0.97846\teval-mlogloss:1.09291\n",
      "[389]\ttrain-mlogloss:0.97826\teval-mlogloss:1.09286\n",
      "[390]\ttrain-mlogloss:0.97802\teval-mlogloss:1.09282\n",
      "[391]\ttrain-mlogloss:0.97781\teval-mlogloss:1.09280\n",
      "[392]\ttrain-mlogloss:0.97757\teval-mlogloss:1.09275\n",
      "[393]\ttrain-mlogloss:0.97737\teval-mlogloss:1.09273\n",
      "[394]\ttrain-mlogloss:0.97721\teval-mlogloss:1.09268\n",
      "[395]\ttrain-mlogloss:0.97696\teval-mlogloss:1.09262\n",
      "[396]\ttrain-mlogloss:0.97664\teval-mlogloss:1.09259\n",
      "[397]\ttrain-mlogloss:0.97635\teval-mlogloss:1.09251\n",
      "[398]\ttrain-mlogloss:0.97611\teval-mlogloss:1.09246\n",
      "[399]\ttrain-mlogloss:0.97584\teval-mlogloss:1.09238\n",
      "[400]\ttrain-mlogloss:0.97560\teval-mlogloss:1.09232\n",
      "[401]\ttrain-mlogloss:0.97526\teval-mlogloss:1.09222\n",
      "[402]\ttrain-mlogloss:0.97499\teval-mlogloss:1.09212\n",
      "[403]\ttrain-mlogloss:0.97474\teval-mlogloss:1.09210\n",
      "[404]\ttrain-mlogloss:0.97451\teval-mlogloss:1.09206\n",
      "[405]\ttrain-mlogloss:0.97424\teval-mlogloss:1.09202\n",
      "[406]\ttrain-mlogloss:0.97402\teval-mlogloss:1.09201\n",
      "[407]\ttrain-mlogloss:0.97370\teval-mlogloss:1.09194\n",
      "[408]\ttrain-mlogloss:0.97343\teval-mlogloss:1.09182\n",
      "[409]\ttrain-mlogloss:0.97316\teval-mlogloss:1.09177\n",
      "[410]\ttrain-mlogloss:0.97293\teval-mlogloss:1.09173\n",
      "[411]\ttrain-mlogloss:0.97265\teval-mlogloss:1.09171\n",
      "[412]\ttrain-mlogloss:0.97236\teval-mlogloss:1.09163\n",
      "[413]\ttrain-mlogloss:0.97210\teval-mlogloss:1.09157\n",
      "[414]\ttrain-mlogloss:0.97180\teval-mlogloss:1.09149\n",
      "[415]\ttrain-mlogloss:0.97159\teval-mlogloss:1.09143\n",
      "[416]\ttrain-mlogloss:0.97136\teval-mlogloss:1.09139\n",
      "[417]\ttrain-mlogloss:0.97112\teval-mlogloss:1.09132\n",
      "[418]\ttrain-mlogloss:0.97095\teval-mlogloss:1.09126\n",
      "[419]\ttrain-mlogloss:0.97070\teval-mlogloss:1.09127\n",
      "[420]\ttrain-mlogloss:0.97043\teval-mlogloss:1.09122\n",
      "[421]\ttrain-mlogloss:0.97015\teval-mlogloss:1.09119\n",
      "[422]\ttrain-mlogloss:0.96994\teval-mlogloss:1.09113\n",
      "[423]\ttrain-mlogloss:0.96970\teval-mlogloss:1.09103\n",
      "[424]\ttrain-mlogloss:0.96947\teval-mlogloss:1.09097\n",
      "[425]\ttrain-mlogloss:0.96922\teval-mlogloss:1.09092\n",
      "[426]\ttrain-mlogloss:0.96901\teval-mlogloss:1.09087\n",
      "[427]\ttrain-mlogloss:0.96879\teval-mlogloss:1.09086\n",
      "[428]\ttrain-mlogloss:0.96857\teval-mlogloss:1.09082\n",
      "[429]\ttrain-mlogloss:0.96831\teval-mlogloss:1.09077\n",
      "[430]\ttrain-mlogloss:0.96812\teval-mlogloss:1.09071\n",
      "[431]\ttrain-mlogloss:0.96789\teval-mlogloss:1.09067\n",
      "[432]\ttrain-mlogloss:0.96764\teval-mlogloss:1.09063\n",
      "[433]\ttrain-mlogloss:0.96743\teval-mlogloss:1.09058\n",
      "[434]\ttrain-mlogloss:0.96718\teval-mlogloss:1.09051\n",
      "[435]\ttrain-mlogloss:0.96698\teval-mlogloss:1.09050\n",
      "[436]\ttrain-mlogloss:0.96680\teval-mlogloss:1.09049\n",
      "[437]\ttrain-mlogloss:0.96654\teval-mlogloss:1.09047\n",
      "[438]\ttrain-mlogloss:0.96636\teval-mlogloss:1.09045\n",
      "[439]\ttrain-mlogloss:0.96612\teval-mlogloss:1.09043\n",
      "[440]\ttrain-mlogloss:0.96592\teval-mlogloss:1.09038\n",
      "[441]\ttrain-mlogloss:0.96566\teval-mlogloss:1.09035\n",
      "[442]\ttrain-mlogloss:0.96543\teval-mlogloss:1.09030\n",
      "[443]\ttrain-mlogloss:0.96525\teval-mlogloss:1.09026\n",
      "[444]\ttrain-mlogloss:0.96498\teval-mlogloss:1.09020\n",
      "[445]\ttrain-mlogloss:0.96477\teval-mlogloss:1.09018\n",
      "[446]\ttrain-mlogloss:0.96456\teval-mlogloss:1.09015\n",
      "[447]\ttrain-mlogloss:0.96434\teval-mlogloss:1.09007\n",
      "[448]\ttrain-mlogloss:0.96415\teval-mlogloss:1.09005\n",
      "[449]\ttrain-mlogloss:0.96394\teval-mlogloss:1.09002\n",
      "[450]\ttrain-mlogloss:0.96373\teval-mlogloss:1.08996\n",
      "[451]\ttrain-mlogloss:0.96352\teval-mlogloss:1.08993\n",
      "[452]\ttrain-mlogloss:0.96327\teval-mlogloss:1.08987\n",
      "[453]\ttrain-mlogloss:0.96309\teval-mlogloss:1.08986\n",
      "[454]\ttrain-mlogloss:0.96287\teval-mlogloss:1.08986\n",
      "[455]\ttrain-mlogloss:0.96263\teval-mlogloss:1.08979\n",
      "[456]\ttrain-mlogloss:0.96233\teval-mlogloss:1.08968\n",
      "[457]\ttrain-mlogloss:0.96211\teval-mlogloss:1.08965\n",
      "[458]\ttrain-mlogloss:0.96188\teval-mlogloss:1.08958\n",
      "[459]\ttrain-mlogloss:0.96172\teval-mlogloss:1.08953\n",
      "[460]\ttrain-mlogloss:0.96147\teval-mlogloss:1.08951\n",
      "[461]\ttrain-mlogloss:0.96122\teval-mlogloss:1.08945\n",
      "[462]\ttrain-mlogloss:0.96096\teval-mlogloss:1.08939\n",
      "[463]\ttrain-mlogloss:0.96077\teval-mlogloss:1.08937\n",
      "[464]\ttrain-mlogloss:0.96061\teval-mlogloss:1.08935\n",
      "[465]\ttrain-mlogloss:0.96046\teval-mlogloss:1.08932\n",
      "[466]\ttrain-mlogloss:0.96030\teval-mlogloss:1.08931\n",
      "[467]\ttrain-mlogloss:0.96012\teval-mlogloss:1.08930\n",
      "[468]\ttrain-mlogloss:0.95993\teval-mlogloss:1.08929\n",
      "[469]\ttrain-mlogloss:0.95969\teval-mlogloss:1.08922\n",
      "[470]\ttrain-mlogloss:0.95943\teval-mlogloss:1.08916\n",
      "[471]\ttrain-mlogloss:0.95925\teval-mlogloss:1.08917\n",
      "[472]\ttrain-mlogloss:0.95905\teval-mlogloss:1.08914\n",
      "[473]\ttrain-mlogloss:0.95884\teval-mlogloss:1.08912\n",
      "[474]\ttrain-mlogloss:0.95862\teval-mlogloss:1.08906\n",
      "[475]\ttrain-mlogloss:0.95839\teval-mlogloss:1.08903\n",
      "[476]\ttrain-mlogloss:0.95815\teval-mlogloss:1.08898\n",
      "[477]\ttrain-mlogloss:0.95792\teval-mlogloss:1.08891\n",
      "[478]\ttrain-mlogloss:0.95770\teval-mlogloss:1.08888\n",
      "[479]\ttrain-mlogloss:0.95747\teval-mlogloss:1.08884\n",
      "[480]\ttrain-mlogloss:0.95727\teval-mlogloss:1.08877\n",
      "[481]\ttrain-mlogloss:0.95710\teval-mlogloss:1.08875\n",
      "[482]\ttrain-mlogloss:0.95688\teval-mlogloss:1.08871\n",
      "[483]\ttrain-mlogloss:0.95670\teval-mlogloss:1.08870\n",
      "[484]\ttrain-mlogloss:0.95649\teval-mlogloss:1.08865\n",
      "[485]\ttrain-mlogloss:0.95627\teval-mlogloss:1.08861\n",
      "[486]\ttrain-mlogloss:0.95604\teval-mlogloss:1.08860\n",
      "[487]\ttrain-mlogloss:0.95588\teval-mlogloss:1.08857\n",
      "[488]\ttrain-mlogloss:0.95569\teval-mlogloss:1.08856\n",
      "[489]\ttrain-mlogloss:0.95548\teval-mlogloss:1.08854\n",
      "[490]\ttrain-mlogloss:0.95522\teval-mlogloss:1.08848\n",
      "[491]\ttrain-mlogloss:0.95504\teval-mlogloss:1.08844\n",
      "[492]\ttrain-mlogloss:0.95478\teval-mlogloss:1.08839\n",
      "[493]\ttrain-mlogloss:0.95461\teval-mlogloss:1.08838\n",
      "[494]\ttrain-mlogloss:0.95435\teval-mlogloss:1.08833\n",
      "[495]\ttrain-mlogloss:0.95419\teval-mlogloss:1.08831\n",
      "[496]\ttrain-mlogloss:0.95400\teval-mlogloss:1.08828\n",
      "[497]\ttrain-mlogloss:0.95376\teval-mlogloss:1.08826\n",
      "[498]\ttrain-mlogloss:0.95355\teval-mlogloss:1.08825\n",
      "[499]\ttrain-mlogloss:0.95333\teval-mlogloss:1.08820\n",
      "[500]\ttrain-mlogloss:0.95307\teval-mlogloss:1.08816\n",
      "[501]\ttrain-mlogloss:0.95285\teval-mlogloss:1.08814\n",
      "[502]\ttrain-mlogloss:0.95263\teval-mlogloss:1.08813\n",
      "[503]\ttrain-mlogloss:0.95237\teval-mlogloss:1.08810\n",
      "[504]\ttrain-mlogloss:0.95214\teval-mlogloss:1.08809\n",
      "[505]\ttrain-mlogloss:0.95194\teval-mlogloss:1.08805\n",
      "[506]\ttrain-mlogloss:0.95176\teval-mlogloss:1.08806\n",
      "[507]\ttrain-mlogloss:0.95154\teval-mlogloss:1.08801\n",
      "[508]\ttrain-mlogloss:0.95138\teval-mlogloss:1.08798\n",
      "[509]\ttrain-mlogloss:0.95114\teval-mlogloss:1.08792\n",
      "[510]\ttrain-mlogloss:0.95090\teval-mlogloss:1.08788\n",
      "[511]\ttrain-mlogloss:0.95068\teval-mlogloss:1.08783\n",
      "[512]\ttrain-mlogloss:0.95046\teval-mlogloss:1.08775\n",
      "[513]\ttrain-mlogloss:0.95027\teval-mlogloss:1.08772\n",
      "[514]\ttrain-mlogloss:0.95005\teval-mlogloss:1.08769\n",
      "[515]\ttrain-mlogloss:0.94984\teval-mlogloss:1.08769\n",
      "[516]\ttrain-mlogloss:0.94966\teval-mlogloss:1.08765\n",
      "[517]\ttrain-mlogloss:0.94944\teval-mlogloss:1.08762\n",
      "[518]\ttrain-mlogloss:0.94922\teval-mlogloss:1.08758\n",
      "[519]\ttrain-mlogloss:0.94901\teval-mlogloss:1.08752\n",
      "[520]\ttrain-mlogloss:0.94881\teval-mlogloss:1.08747\n",
      "[521]\ttrain-mlogloss:0.94855\teval-mlogloss:1.08740\n",
      "[522]\ttrain-mlogloss:0.94836\teval-mlogloss:1.08738\n",
      "[523]\ttrain-mlogloss:0.94816\teval-mlogloss:1.08738\n",
      "[524]\ttrain-mlogloss:0.94797\teval-mlogloss:1.08738\n",
      "[525]\ttrain-mlogloss:0.94776\teval-mlogloss:1.08735\n",
      "[526]\ttrain-mlogloss:0.94756\teval-mlogloss:1.08734\n",
      "[527]\ttrain-mlogloss:0.94738\teval-mlogloss:1.08733\n",
      "[528]\ttrain-mlogloss:0.94716\teval-mlogloss:1.08731\n",
      "[529]\ttrain-mlogloss:0.94691\teval-mlogloss:1.08725\n",
      "[530]\ttrain-mlogloss:0.94668\teval-mlogloss:1.08720\n",
      "[531]\ttrain-mlogloss:0.94647\teval-mlogloss:1.08715\n",
      "[532]\ttrain-mlogloss:0.94622\teval-mlogloss:1.08709\n",
      "[533]\ttrain-mlogloss:0.94602\teval-mlogloss:1.08708\n",
      "[534]\ttrain-mlogloss:0.94584\teval-mlogloss:1.08708\n",
      "[535]\ttrain-mlogloss:0.94560\teval-mlogloss:1.08709\n",
      "[536]\ttrain-mlogloss:0.94537\teval-mlogloss:1.08705\n",
      "[537]\ttrain-mlogloss:0.94516\teval-mlogloss:1.08702\n",
      "[538]\ttrain-mlogloss:0.94492\teval-mlogloss:1.08695\n",
      "[539]\ttrain-mlogloss:0.94463\teval-mlogloss:1.08686\n",
      "[540]\ttrain-mlogloss:0.94438\teval-mlogloss:1.08678\n",
      "[541]\ttrain-mlogloss:0.94419\teval-mlogloss:1.08674\n",
      "[542]\ttrain-mlogloss:0.94393\teval-mlogloss:1.08667\n",
      "[543]\ttrain-mlogloss:0.94369\teval-mlogloss:1.08659\n",
      "[544]\ttrain-mlogloss:0.94340\teval-mlogloss:1.08650\n",
      "[545]\ttrain-mlogloss:0.94319\teval-mlogloss:1.08644\n",
      "[546]\ttrain-mlogloss:0.94297\teval-mlogloss:1.08640\n",
      "[547]\ttrain-mlogloss:0.94272\teval-mlogloss:1.08633\n",
      "[548]\ttrain-mlogloss:0.94245\teval-mlogloss:1.08627\n",
      "[549]\ttrain-mlogloss:0.94227\teval-mlogloss:1.08627\n",
      "[550]\ttrain-mlogloss:0.94204\teval-mlogloss:1.08622\n",
      "[551]\ttrain-mlogloss:0.94181\teval-mlogloss:1.08619\n",
      "[552]\ttrain-mlogloss:0.94163\teval-mlogloss:1.08613\n",
      "[553]\ttrain-mlogloss:0.94139\teval-mlogloss:1.08608\n",
      "[554]\ttrain-mlogloss:0.94116\teval-mlogloss:1.08605\n",
      "[555]\ttrain-mlogloss:0.94091\teval-mlogloss:1.08601\n",
      "[556]\ttrain-mlogloss:0.94074\teval-mlogloss:1.08598\n",
      "[557]\ttrain-mlogloss:0.94058\teval-mlogloss:1.08596\n",
      "[558]\ttrain-mlogloss:0.94035\teval-mlogloss:1.08592\n",
      "[559]\ttrain-mlogloss:0.94013\teval-mlogloss:1.08594\n",
      "[560]\ttrain-mlogloss:0.93990\teval-mlogloss:1.08592\n",
      "[561]\ttrain-mlogloss:0.93970\teval-mlogloss:1.08592\n",
      "[562]\ttrain-mlogloss:0.93947\teval-mlogloss:1.08586\n",
      "[563]\ttrain-mlogloss:0.93936\teval-mlogloss:1.08584\n",
      "[564]\ttrain-mlogloss:0.93916\teval-mlogloss:1.08582\n",
      "[565]\ttrain-mlogloss:0.93896\teval-mlogloss:1.08580\n",
      "[566]\ttrain-mlogloss:0.93875\teval-mlogloss:1.08580\n",
      "[567]\ttrain-mlogloss:0.93850\teval-mlogloss:1.08577\n",
      "[568]\ttrain-mlogloss:0.93829\teval-mlogloss:1.08571\n",
      "[569]\ttrain-mlogloss:0.93808\teval-mlogloss:1.08568\n",
      "[570]\ttrain-mlogloss:0.93784\teval-mlogloss:1.08567\n",
      "[571]\ttrain-mlogloss:0.93762\teval-mlogloss:1.08562\n",
      "[572]\ttrain-mlogloss:0.93740\teval-mlogloss:1.08553\n",
      "[573]\ttrain-mlogloss:0.93722\teval-mlogloss:1.08551\n",
      "[574]\ttrain-mlogloss:0.93699\teval-mlogloss:1.08544\n",
      "[575]\ttrain-mlogloss:0.93675\teval-mlogloss:1.08542\n",
      "[576]\ttrain-mlogloss:0.93658\teval-mlogloss:1.08539\n",
      "[577]\ttrain-mlogloss:0.93638\teval-mlogloss:1.08536\n",
      "[578]\ttrain-mlogloss:0.93620\teval-mlogloss:1.08532\n",
      "[579]\ttrain-mlogloss:0.93605\teval-mlogloss:1.08530\n",
      "[580]\ttrain-mlogloss:0.93588\teval-mlogloss:1.08531\n",
      "[581]\ttrain-mlogloss:0.93565\teval-mlogloss:1.08529\n",
      "[582]\ttrain-mlogloss:0.93548\teval-mlogloss:1.08527\n",
      "[583]\ttrain-mlogloss:0.93532\teval-mlogloss:1.08525\n",
      "[584]\ttrain-mlogloss:0.93514\teval-mlogloss:1.08524\n",
      "[585]\ttrain-mlogloss:0.93496\teval-mlogloss:1.08523\n",
      "[586]\ttrain-mlogloss:0.93476\teval-mlogloss:1.08515\n",
      "[587]\ttrain-mlogloss:0.93458\teval-mlogloss:1.08511\n",
      "[588]\ttrain-mlogloss:0.93440\teval-mlogloss:1.08508\n",
      "[589]\ttrain-mlogloss:0.93418\teval-mlogloss:1.08501\n",
      "[590]\ttrain-mlogloss:0.93395\teval-mlogloss:1.08495\n",
      "[591]\ttrain-mlogloss:0.93370\teval-mlogloss:1.08490\n",
      "[592]\ttrain-mlogloss:0.93349\teval-mlogloss:1.08489\n",
      "[593]\ttrain-mlogloss:0.93328\teval-mlogloss:1.08485\n",
      "[594]\ttrain-mlogloss:0.93314\teval-mlogloss:1.08485\n",
      "[595]\ttrain-mlogloss:0.93292\teval-mlogloss:1.08480\n",
      "[596]\ttrain-mlogloss:0.93267\teval-mlogloss:1.08469\n",
      "[597]\ttrain-mlogloss:0.93249\teval-mlogloss:1.08463\n",
      "[598]\ttrain-mlogloss:0.93225\teval-mlogloss:1.08460\n",
      "[599]\ttrain-mlogloss:0.93207\teval-mlogloss:1.08457\n",
      "[600]\ttrain-mlogloss:0.93184\teval-mlogloss:1.08458\n",
      "[601]\ttrain-mlogloss:0.93161\teval-mlogloss:1.08458\n",
      "[602]\ttrain-mlogloss:0.93146\teval-mlogloss:1.08456\n",
      "[603]\ttrain-mlogloss:0.93132\teval-mlogloss:1.08452\n",
      "[604]\ttrain-mlogloss:0.93117\teval-mlogloss:1.08447\n",
      "[605]\ttrain-mlogloss:0.93100\teval-mlogloss:1.08442\n",
      "[606]\ttrain-mlogloss:0.93087\teval-mlogloss:1.08440\n",
      "[607]\ttrain-mlogloss:0.93069\teval-mlogloss:1.08437\n",
      "[608]\ttrain-mlogloss:0.93046\teval-mlogloss:1.08433\n",
      "[609]\ttrain-mlogloss:0.93024\teval-mlogloss:1.08433\n",
      "[610]\ttrain-mlogloss:0.93004\teval-mlogloss:1.08430\n",
      "[611]\ttrain-mlogloss:0.92980\teval-mlogloss:1.08429\n",
      "[612]\ttrain-mlogloss:0.92961\teval-mlogloss:1.08427\n",
      "[613]\ttrain-mlogloss:0.92942\teval-mlogloss:1.08425\n",
      "[614]\ttrain-mlogloss:0.92930\teval-mlogloss:1.08424\n",
      "[615]\ttrain-mlogloss:0.92909\teval-mlogloss:1.08422\n",
      "[616]\ttrain-mlogloss:0.92896\teval-mlogloss:1.08424\n",
      "[617]\ttrain-mlogloss:0.92871\teval-mlogloss:1.08420\n",
      "[618]\ttrain-mlogloss:0.92855\teval-mlogloss:1.08422\n",
      "[619]\ttrain-mlogloss:0.92836\teval-mlogloss:1.08419\n",
      "[620]\ttrain-mlogloss:0.92818\teval-mlogloss:1.08415\n",
      "[621]\ttrain-mlogloss:0.92804\teval-mlogloss:1.08413\n",
      "[622]\ttrain-mlogloss:0.92787\teval-mlogloss:1.08411\n",
      "[623]\ttrain-mlogloss:0.92770\teval-mlogloss:1.08412\n",
      "[624]\ttrain-mlogloss:0.92753\teval-mlogloss:1.08410\n",
      "[625]\ttrain-mlogloss:0.92735\teval-mlogloss:1.08406\n",
      "[626]\ttrain-mlogloss:0.92718\teval-mlogloss:1.08403\n",
      "[627]\ttrain-mlogloss:0.92700\teval-mlogloss:1.08401\n",
      "[628]\ttrain-mlogloss:0.92683\teval-mlogloss:1.08402\n",
      "[629]\ttrain-mlogloss:0.92663\teval-mlogloss:1.08398\n",
      "[630]\ttrain-mlogloss:0.92642\teval-mlogloss:1.08393\n",
      "[631]\ttrain-mlogloss:0.92625\teval-mlogloss:1.08391\n",
      "[632]\ttrain-mlogloss:0.92601\teval-mlogloss:1.08387\n",
      "[633]\ttrain-mlogloss:0.92586\teval-mlogloss:1.08383\n",
      "[634]\ttrain-mlogloss:0.92570\teval-mlogloss:1.08381\n",
      "[635]\ttrain-mlogloss:0.92549\teval-mlogloss:1.08383\n",
      "[636]\ttrain-mlogloss:0.92532\teval-mlogloss:1.08381\n",
      "[637]\ttrain-mlogloss:0.92518\teval-mlogloss:1.08376\n",
      "[638]\ttrain-mlogloss:0.92501\teval-mlogloss:1.08372\n",
      "[639]\ttrain-mlogloss:0.92478\teval-mlogloss:1.08369\n",
      "[640]\ttrain-mlogloss:0.92457\teval-mlogloss:1.08364\n",
      "[641]\ttrain-mlogloss:0.92439\teval-mlogloss:1.08362\n",
      "[642]\ttrain-mlogloss:0.92422\teval-mlogloss:1.08360\n",
      "[643]\ttrain-mlogloss:0.92403\teval-mlogloss:1.08359\n",
      "[644]\ttrain-mlogloss:0.92385\teval-mlogloss:1.08360\n",
      "[645]\ttrain-mlogloss:0.92369\teval-mlogloss:1.08358\n",
      "[646]\ttrain-mlogloss:0.92353\teval-mlogloss:1.08358\n",
      "[647]\ttrain-mlogloss:0.92335\teval-mlogloss:1.08357\n",
      "[648]\ttrain-mlogloss:0.92318\teval-mlogloss:1.08354\n",
      "[649]\ttrain-mlogloss:0.92301\teval-mlogloss:1.08353\n",
      "[650]\ttrain-mlogloss:0.92277\teval-mlogloss:1.08344\n",
      "[651]\ttrain-mlogloss:0.92261\teval-mlogloss:1.08343\n",
      "[652]\ttrain-mlogloss:0.92241\teval-mlogloss:1.08341\n",
      "[653]\ttrain-mlogloss:0.92219\teval-mlogloss:1.08338\n",
      "[654]\ttrain-mlogloss:0.92199\teval-mlogloss:1.08337\n",
      "[655]\ttrain-mlogloss:0.92181\teval-mlogloss:1.08333\n",
      "[656]\ttrain-mlogloss:0.92166\teval-mlogloss:1.08331\n",
      "[657]\ttrain-mlogloss:0.92148\teval-mlogloss:1.08330\n",
      "[658]\ttrain-mlogloss:0.92127\teval-mlogloss:1.08328\n",
      "[659]\ttrain-mlogloss:0.92100\teval-mlogloss:1.08326\n",
      "[660]\ttrain-mlogloss:0.92079\teval-mlogloss:1.08320\n",
      "[661]\ttrain-mlogloss:0.92053\teval-mlogloss:1.08317\n",
      "[662]\ttrain-mlogloss:0.92036\teval-mlogloss:1.08314\n",
      "[663]\ttrain-mlogloss:0.92018\teval-mlogloss:1.08312\n",
      "[664]\ttrain-mlogloss:0.91998\teval-mlogloss:1.08312\n",
      "[665]\ttrain-mlogloss:0.91980\teval-mlogloss:1.08310\n",
      "[666]\ttrain-mlogloss:0.91964\teval-mlogloss:1.08311\n",
      "[667]\ttrain-mlogloss:0.91953\teval-mlogloss:1.08312\n",
      "[668]\ttrain-mlogloss:0.91933\teval-mlogloss:1.08308\n",
      "[669]\ttrain-mlogloss:0.91912\teval-mlogloss:1.08305\n",
      "[670]\ttrain-mlogloss:0.91900\teval-mlogloss:1.08304\n",
      "[671]\ttrain-mlogloss:0.91887\teval-mlogloss:1.08305\n",
      "[672]\ttrain-mlogloss:0.91870\teval-mlogloss:1.08304\n",
      "[673]\ttrain-mlogloss:0.91848\teval-mlogloss:1.08302\n",
      "[674]\ttrain-mlogloss:0.91830\teval-mlogloss:1.08299\n",
      "[675]\ttrain-mlogloss:0.91812\teval-mlogloss:1.08296\n",
      "[676]\ttrain-mlogloss:0.91795\teval-mlogloss:1.08301\n",
      "[677]\ttrain-mlogloss:0.91780\teval-mlogloss:1.08298\n",
      "[678]\ttrain-mlogloss:0.91767\teval-mlogloss:1.08295\n",
      "[679]\ttrain-mlogloss:0.91747\teval-mlogloss:1.08294\n",
      "[680]\ttrain-mlogloss:0.91728\teval-mlogloss:1.08298\n",
      "[681]\ttrain-mlogloss:0.91713\teval-mlogloss:1.08296\n",
      "[682]\ttrain-mlogloss:0.91698\teval-mlogloss:1.08294\n",
      "[683]\ttrain-mlogloss:0.91675\teval-mlogloss:1.08287\n",
      "[684]\ttrain-mlogloss:0.91658\teval-mlogloss:1.08287\n",
      "[685]\ttrain-mlogloss:0.91641\teval-mlogloss:1.08282\n",
      "[686]\ttrain-mlogloss:0.91630\teval-mlogloss:1.08281\n",
      "[687]\ttrain-mlogloss:0.91610\teval-mlogloss:1.08280\n",
      "[688]\ttrain-mlogloss:0.91600\teval-mlogloss:1.08280\n",
      "[689]\ttrain-mlogloss:0.91583\teval-mlogloss:1.08278\n",
      "[690]\ttrain-mlogloss:0.91569\teval-mlogloss:1.08278\n",
      "[691]\ttrain-mlogloss:0.91552\teval-mlogloss:1.08275\n",
      "[692]\ttrain-mlogloss:0.91534\teval-mlogloss:1.08275\n",
      "[693]\ttrain-mlogloss:0.91515\teval-mlogloss:1.08271\n",
      "[694]\ttrain-mlogloss:0.91499\teval-mlogloss:1.08269\n",
      "[695]\ttrain-mlogloss:0.91481\teval-mlogloss:1.08266\n",
      "[696]\ttrain-mlogloss:0.91460\teval-mlogloss:1.08261\n",
      "[697]\ttrain-mlogloss:0.91442\teval-mlogloss:1.08260\n",
      "[698]\ttrain-mlogloss:0.91427\teval-mlogloss:1.08259\n",
      "[699]\ttrain-mlogloss:0.91410\teval-mlogloss:1.08256\n",
      "[700]\ttrain-mlogloss:0.91389\teval-mlogloss:1.08257\n",
      "[701]\ttrain-mlogloss:0.91370\teval-mlogloss:1.08254\n",
      "[702]\ttrain-mlogloss:0.91353\teval-mlogloss:1.08255\n",
      "[703]\ttrain-mlogloss:0.91332\teval-mlogloss:1.08252\n",
      "[704]\ttrain-mlogloss:0.91317\teval-mlogloss:1.08250\n",
      "[705]\ttrain-mlogloss:0.91294\teval-mlogloss:1.08245\n",
      "[706]\ttrain-mlogloss:0.91279\teval-mlogloss:1.08245\n",
      "[707]\ttrain-mlogloss:0.91263\teval-mlogloss:1.08247\n",
      "[708]\ttrain-mlogloss:0.91247\teval-mlogloss:1.08248\n",
      "[709]\ttrain-mlogloss:0.91231\teval-mlogloss:1.08250\n",
      "[710]\ttrain-mlogloss:0.91218\teval-mlogloss:1.08251\n",
      "[711]\ttrain-mlogloss:0.91206\teval-mlogloss:1.08249\n",
      "[712]\ttrain-mlogloss:0.91191\teval-mlogloss:1.08249\n",
      "[713]\ttrain-mlogloss:0.91176\teval-mlogloss:1.08248\n",
      "[714]\ttrain-mlogloss:0.91165\teval-mlogloss:1.08251\n",
      "[715]\ttrain-mlogloss:0.91150\teval-mlogloss:1.08249\n",
      "[716]\ttrain-mlogloss:0.91137\teval-mlogloss:1.08247\n",
      "[717]\ttrain-mlogloss:0.91119\teval-mlogloss:1.08241\n",
      "[718]\ttrain-mlogloss:0.91103\teval-mlogloss:1.08237\n",
      "[719]\ttrain-mlogloss:0.91090\teval-mlogloss:1.08237\n",
      "[720]\ttrain-mlogloss:0.91076\teval-mlogloss:1.08234\n",
      "[721]\ttrain-mlogloss:0.91063\teval-mlogloss:1.08236\n",
      "[722]\ttrain-mlogloss:0.91050\teval-mlogloss:1.08234\n",
      "[723]\ttrain-mlogloss:0.91035\teval-mlogloss:1.08231\n",
      "[724]\ttrain-mlogloss:0.91019\teval-mlogloss:1.08233\n",
      "[725]\ttrain-mlogloss:0.91004\teval-mlogloss:1.08230\n",
      "[726]\ttrain-mlogloss:0.90992\teval-mlogloss:1.08229\n",
      "[727]\ttrain-mlogloss:0.90976\teval-mlogloss:1.08231\n",
      "[728]\ttrain-mlogloss:0.90961\teval-mlogloss:1.08226\n",
      "[729]\ttrain-mlogloss:0.90946\teval-mlogloss:1.08225\n",
      "[730]\ttrain-mlogloss:0.90934\teval-mlogloss:1.08222\n",
      "[731]\ttrain-mlogloss:0.90916\teval-mlogloss:1.08218\n",
      "[732]\ttrain-mlogloss:0.90901\teval-mlogloss:1.08218\n",
      "[733]\ttrain-mlogloss:0.90887\teval-mlogloss:1.08218\n",
      "[734]\ttrain-mlogloss:0.90872\teval-mlogloss:1.08218\n",
      "[735]\ttrain-mlogloss:0.90856\teval-mlogloss:1.08218\n",
      "[736]\ttrain-mlogloss:0.90837\teval-mlogloss:1.08216\n",
      "[737]\ttrain-mlogloss:0.90821\teval-mlogloss:1.08216\n",
      "[738]\ttrain-mlogloss:0.90807\teval-mlogloss:1.08217\n",
      "[739]\ttrain-mlogloss:0.90792\teval-mlogloss:1.08218\n",
      "[740]\ttrain-mlogloss:0.90777\teval-mlogloss:1.08217\n",
      "[741]\ttrain-mlogloss:0.90762\teval-mlogloss:1.08215\n",
      "[742]\ttrain-mlogloss:0.90744\teval-mlogloss:1.08214\n",
      "[743]\ttrain-mlogloss:0.90723\teval-mlogloss:1.08211\n",
      "[744]\ttrain-mlogloss:0.90703\teval-mlogloss:1.08208\n",
      "[745]\ttrain-mlogloss:0.90685\teval-mlogloss:1.08205\n",
      "[746]\ttrain-mlogloss:0.90667\teval-mlogloss:1.08199\n",
      "[747]\ttrain-mlogloss:0.90647\teval-mlogloss:1.08191\n",
      "[748]\ttrain-mlogloss:0.90631\teval-mlogloss:1.08193\n",
      "[749]\ttrain-mlogloss:0.90614\teval-mlogloss:1.08190\n",
      "[750]\ttrain-mlogloss:0.90590\teval-mlogloss:1.08184\n",
      "[751]\ttrain-mlogloss:0.90570\teval-mlogloss:1.08181\n",
      "[752]\ttrain-mlogloss:0.90552\teval-mlogloss:1.08176\n",
      "[753]\ttrain-mlogloss:0.90534\teval-mlogloss:1.08176\n",
      "[754]\ttrain-mlogloss:0.90515\teval-mlogloss:1.08175\n",
      "[755]\ttrain-mlogloss:0.90498\teval-mlogloss:1.08172\n",
      "[756]\ttrain-mlogloss:0.90480\teval-mlogloss:1.08171\n",
      "[757]\ttrain-mlogloss:0.90462\teval-mlogloss:1.08168\n",
      "[758]\ttrain-mlogloss:0.90445\teval-mlogloss:1.08165\n",
      "[759]\ttrain-mlogloss:0.90425\teval-mlogloss:1.08164\n",
      "[760]\ttrain-mlogloss:0.90408\teval-mlogloss:1.08160\n",
      "[761]\ttrain-mlogloss:0.90387\teval-mlogloss:1.08159\n",
      "[762]\ttrain-mlogloss:0.90368\teval-mlogloss:1.08156\n",
      "[763]\ttrain-mlogloss:0.90352\teval-mlogloss:1.08157\n",
      "[764]\ttrain-mlogloss:0.90337\teval-mlogloss:1.08155\n",
      "[765]\ttrain-mlogloss:0.90322\teval-mlogloss:1.08154\n",
      "[766]\ttrain-mlogloss:0.90308\teval-mlogloss:1.08150\n",
      "[767]\ttrain-mlogloss:0.90297\teval-mlogloss:1.08152\n",
      "[768]\ttrain-mlogloss:0.90284\teval-mlogloss:1.08151\n",
      "[769]\ttrain-mlogloss:0.90269\teval-mlogloss:1.08149\n",
      "[770]\ttrain-mlogloss:0.90257\teval-mlogloss:1.08146\n",
      "[771]\ttrain-mlogloss:0.90244\teval-mlogloss:1.08144\n",
      "[772]\ttrain-mlogloss:0.90231\teval-mlogloss:1.08143\n",
      "[773]\ttrain-mlogloss:0.90215\teval-mlogloss:1.08140\n",
      "[774]\ttrain-mlogloss:0.90200\teval-mlogloss:1.08138\n",
      "[775]\ttrain-mlogloss:0.90187\teval-mlogloss:1.08138\n",
      "[776]\ttrain-mlogloss:0.90174\teval-mlogloss:1.08138\n",
      "[777]\ttrain-mlogloss:0.90160\teval-mlogloss:1.08140\n",
      "[778]\ttrain-mlogloss:0.90149\teval-mlogloss:1.08142\n",
      "[779]\ttrain-mlogloss:0.90134\teval-mlogloss:1.08141\n",
      "[780]\ttrain-mlogloss:0.90113\teval-mlogloss:1.08137\n",
      "[781]\ttrain-mlogloss:0.90098\teval-mlogloss:1.08135\n",
      "[782]\ttrain-mlogloss:0.90078\teval-mlogloss:1.08135\n",
      "[783]\ttrain-mlogloss:0.90060\teval-mlogloss:1.08134\n",
      "[784]\ttrain-mlogloss:0.90043\teval-mlogloss:1.08133\n",
      "[785]\ttrain-mlogloss:0.90027\teval-mlogloss:1.08127\n",
      "[786]\ttrain-mlogloss:0.90010\teval-mlogloss:1.08126\n",
      "[787]\ttrain-mlogloss:0.89991\teval-mlogloss:1.08121\n",
      "[788]\ttrain-mlogloss:0.89977\teval-mlogloss:1.08122\n",
      "[789]\ttrain-mlogloss:0.89959\teval-mlogloss:1.08118\n",
      "[790]\ttrain-mlogloss:0.89944\teval-mlogloss:1.08118\n",
      "[791]\ttrain-mlogloss:0.89930\teval-mlogloss:1.08118\n",
      "[792]\ttrain-mlogloss:0.89919\teval-mlogloss:1.08116\n",
      "[793]\ttrain-mlogloss:0.89900\teval-mlogloss:1.08114\n",
      "[794]\ttrain-mlogloss:0.89886\teval-mlogloss:1.08112\n",
      "[795]\ttrain-mlogloss:0.89865\teval-mlogloss:1.08109\n",
      "[796]\ttrain-mlogloss:0.89845\teval-mlogloss:1.08107\n",
      "[797]\ttrain-mlogloss:0.89828\teval-mlogloss:1.08105\n",
      "[798]\ttrain-mlogloss:0.89808\teval-mlogloss:1.08104\n",
      "[799]\ttrain-mlogloss:0.89793\teval-mlogloss:1.08104\n",
      "[800]\ttrain-mlogloss:0.89777\teval-mlogloss:1.08103\n",
      "[801]\ttrain-mlogloss:0.89764\teval-mlogloss:1.08100\n",
      "[802]\ttrain-mlogloss:0.89746\teval-mlogloss:1.08096\n",
      "[803]\ttrain-mlogloss:0.89735\teval-mlogloss:1.08096\n",
      "[804]\ttrain-mlogloss:0.89719\teval-mlogloss:1.08093\n",
      "[805]\ttrain-mlogloss:0.89702\teval-mlogloss:1.08090\n",
      "[806]\ttrain-mlogloss:0.89690\teval-mlogloss:1.08087\n",
      "[807]\ttrain-mlogloss:0.89675\teval-mlogloss:1.08084\n",
      "[808]\ttrain-mlogloss:0.89659\teval-mlogloss:1.08084\n",
      "[809]\ttrain-mlogloss:0.89643\teval-mlogloss:1.08084\n",
      "[810]\ttrain-mlogloss:0.89627\teval-mlogloss:1.08083\n",
      "[811]\ttrain-mlogloss:0.89609\teval-mlogloss:1.08082\n",
      "[812]\ttrain-mlogloss:0.89594\teval-mlogloss:1.08081\n",
      "[813]\ttrain-mlogloss:0.89579\teval-mlogloss:1.08080\n",
      "[814]\ttrain-mlogloss:0.89564\teval-mlogloss:1.08077\n",
      "[815]\ttrain-mlogloss:0.89553\teval-mlogloss:1.08077\n",
      "[816]\ttrain-mlogloss:0.89537\teval-mlogloss:1.08072\n",
      "[817]\ttrain-mlogloss:0.89521\teval-mlogloss:1.08069\n",
      "[818]\ttrain-mlogloss:0.89504\teval-mlogloss:1.08068\n",
      "[819]\ttrain-mlogloss:0.89485\teval-mlogloss:1.08066\n",
      "[820]\ttrain-mlogloss:0.89470\teval-mlogloss:1.08068\n",
      "[821]\ttrain-mlogloss:0.89455\teval-mlogloss:1.08067\n",
      "[822]\ttrain-mlogloss:0.89436\teval-mlogloss:1.08067\n",
      "[823]\ttrain-mlogloss:0.89421\teval-mlogloss:1.08064\n",
      "[824]\ttrain-mlogloss:0.89406\teval-mlogloss:1.08065\n",
      "[825]\ttrain-mlogloss:0.89394\teval-mlogloss:1.08064\n",
      "[826]\ttrain-mlogloss:0.89379\teval-mlogloss:1.08063\n",
      "[827]\ttrain-mlogloss:0.89364\teval-mlogloss:1.08064\n",
      "[828]\ttrain-mlogloss:0.89348\teval-mlogloss:1.08062\n",
      "[829]\ttrain-mlogloss:0.89333\teval-mlogloss:1.08063\n",
      "[830]\ttrain-mlogloss:0.89310\teval-mlogloss:1.08059\n",
      "[831]\ttrain-mlogloss:0.89293\teval-mlogloss:1.08060\n",
      "[832]\ttrain-mlogloss:0.89278\teval-mlogloss:1.08061\n",
      "[833]\ttrain-mlogloss:0.89258\teval-mlogloss:1.08060\n",
      "[834]\ttrain-mlogloss:0.89243\teval-mlogloss:1.08059\n",
      "[835]\ttrain-mlogloss:0.89226\teval-mlogloss:1.08061\n",
      "[836]\ttrain-mlogloss:0.89213\teval-mlogloss:1.08062\n",
      "[837]\ttrain-mlogloss:0.89200\teval-mlogloss:1.08059\n",
      "[838]\ttrain-mlogloss:0.89188\teval-mlogloss:1.08059\n",
      "[839]\ttrain-mlogloss:0.89173\teval-mlogloss:1.08057\n",
      "[840]\ttrain-mlogloss:0.89157\teval-mlogloss:1.08056\n",
      "[841]\ttrain-mlogloss:0.89141\teval-mlogloss:1.08056\n",
      "[842]\ttrain-mlogloss:0.89126\teval-mlogloss:1.08054\n",
      "[843]\ttrain-mlogloss:0.89112\teval-mlogloss:1.08059\n",
      "[844]\ttrain-mlogloss:0.89101\teval-mlogloss:1.08062\n",
      "[845]\ttrain-mlogloss:0.89085\teval-mlogloss:1.08060\n",
      "[846]\ttrain-mlogloss:0.89068\teval-mlogloss:1.08059\n",
      "[847]\ttrain-mlogloss:0.89056\teval-mlogloss:1.08057\n",
      "[848]\ttrain-mlogloss:0.89044\teval-mlogloss:1.08057\n",
      "[849]\ttrain-mlogloss:0.89029\teval-mlogloss:1.08055\n",
      "[850]\ttrain-mlogloss:0.89013\teval-mlogloss:1.08053\n",
      "[851]\ttrain-mlogloss:0.88994\teval-mlogloss:1.08050\n",
      "[852]\ttrain-mlogloss:0.88979\teval-mlogloss:1.08050\n",
      "[853]\ttrain-mlogloss:0.88966\teval-mlogloss:1.08050\n",
      "[854]\ttrain-mlogloss:0.88954\teval-mlogloss:1.08049\n",
      "[855]\ttrain-mlogloss:0.88939\teval-mlogloss:1.08051\n",
      "[856]\ttrain-mlogloss:0.88925\teval-mlogloss:1.08051\n",
      "[857]\ttrain-mlogloss:0.88914\teval-mlogloss:1.08054\n",
      "[858]\ttrain-mlogloss:0.88903\teval-mlogloss:1.08055\n",
      "[859]\ttrain-mlogloss:0.88889\teval-mlogloss:1.08054\n",
      "[860]\ttrain-mlogloss:0.88878\teval-mlogloss:1.08059\n",
      "[861]\ttrain-mlogloss:0.88869\teval-mlogloss:1.08060\n",
      "[862]\ttrain-mlogloss:0.88857\teval-mlogloss:1.08058\n",
      "[863]\ttrain-mlogloss:0.88841\teval-mlogloss:1.08057\n",
      "[864]\ttrain-mlogloss:0.88824\teval-mlogloss:1.08055\n",
      "[865]\ttrain-mlogloss:0.88808\teval-mlogloss:1.08053\n",
      "[866]\ttrain-mlogloss:0.88791\teval-mlogloss:1.08054\n",
      "[867]\ttrain-mlogloss:0.88777\teval-mlogloss:1.08054\n",
      "[868]\ttrain-mlogloss:0.88760\teval-mlogloss:1.08052\n",
      "[869]\ttrain-mlogloss:0.88747\teval-mlogloss:1.08053\n",
      "[870]\ttrain-mlogloss:0.88736\teval-mlogloss:1.08054\n",
      "[871]\ttrain-mlogloss:0.88721\teval-mlogloss:1.08054\n",
      "[872]\ttrain-mlogloss:0.88701\teval-mlogloss:1.08055\n",
      "[873]\ttrain-mlogloss:0.88684\teval-mlogloss:1.08055\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Construct the parameter for the XGB Model\n",
    "param = {\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': 8,\n",
    "    'nthread': 4,\n",
    "    'num_class': len(prods),\n",
    "    'objective': 'multi:softprob',\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'eta': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.9,\n",
    "    'seed': 2018,\n",
    "    }\n",
    "\n",
    "# Convert training and validation data to XGBoost format.\n",
    "X_trn = XY_trn[features].values\n",
    "Y_trn = XY_trn['y'].values\n",
    "dtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)\n",
    "\n",
    "X_vld = XY_vld[features].values\n",
    "Y_vld = XY_vld['y'].values\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
    "\n",
    "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
    "model = xgb.train(param, dtrn, num_boost_round=1000, evals=watch_list, early_stopping_rounds=20)\n",
    "\n",
    "# Save model in the pickle for easy loading for the future\n",
    "import pickle\n",
    "pickle.dump(model, open(\"./model/xgb.baseline.pkl\", \"wb\"))\n",
    "best_ntree_limit = model.best_ntree_limit\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cross-validation, the performance level is confirmed using MAP@7, the evaluation scale of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "\n",
    "# average precision\n",
    "def apk(actual, predicted, k=7, default=0.0):\n",
    "    # Since it is MAP@7, we use max 7 items to calculate\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        # Rules to give a score:\n",
    "        # Predicted is in the actual (p in actual)\n",
    "        # predicted is not a duplicate (p no in predicted[:i])\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    # if actual is empty, it return 0\n",
    "    if not actual:\n",
    "        return default\n",
    "\n",
    "    # use number of actual to calculkate the average precision\n",
    "    return score / min(len(actual) , k)\n",
    "\n",
    "# mean average precision\n",
    "def mapk(actual, predicted, k=7, default=0.0):\n",
    "    # actual and predicted are list of list. \n",
    "    # for each customer, get the average precision and get mean using np.mean()\n",
    "    return np.mean([apk(a, p, k, default) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n",
      "C:\\Users\\Yuchie\\AppData\\Local\\Temp\\ipykernel_12896\\1234760100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vld[padd] = vld[prod] - vld[prev]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04266379915553903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yuchie\\Desktop\\DataScience\\santander\\Lib\\site-packages\\xgboost\\core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012745271121263942\n"
     ]
    }
   ],
   "source": [
    "# Code below is to selectin top 7 predictions for MAP@7 evalution metrics\n",
    "# Extract customer identification number.\n",
    "vld = trn[trn['fecha_dato'] == vld_date]\n",
    "ncodpers_vld = vld['ncodpers'].values\n",
    "# Retrieve new purchases from validation data.\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    padd = prod + '_add'\n",
    "    vld[padd] = vld[prod] - vld[prev]    \n",
    "add_vld = vld[[prod + '_add' for prod in prods]].values\n",
    "add_vld_list = [list() for i in range(len(ncodpers_vld))]\n",
    "\n",
    "# The new purchase correct answer value for each customer is stored in add_vld_list,\n",
    "# and the total count is stored in count_vld\n",
    "count_vld = 0\n",
    "for ncodper in range(len(ncodpers_vld)):\n",
    "    for prod in range(len(prods)):\n",
    "        if add_vld[ncodper, prod] > 0:\n",
    "            add_vld_list[ncodper].append(prod)\n",
    "            count_vld += 1\n",
    "\n",
    "# The highest score of MAP@7 that can be obtained from the verification data is obtained in advance\n",
    "print(mapk(add_vld_list, add_vld_list, 7, 0.0))\n",
    "\n",
    "# Get the predicted value for the validation data.\n",
    "X_vld = XY_vld[features].values\n",
    "Y_vld = XY_vld['y'].values\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
    "preds_vld = model.predict(dvld, ntree_limit=best_ntree_limit)\n",
    "\n",
    "# Since new purchases are not possible for products held last month, subtract 1 from the predicted value in advance.\n",
    "preds_vld = preds_vld - XY_vld[[prod + '_prev' for prod in prods]].values\n",
    "\n",
    "# Extract the top 7 prediction data from validation data\n",
    "result_vld = []\n",
    "for ncodper, pred in zip(ncodpers_vld, preds_vld):\n",
    "    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    result_vld.append([ip for y,p,ip in y_prods])\n",
    "    \n",
    "# MAP@7 for validation\n",
    "print(mapk(add_vld_list, result_vld, 7, 0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>sexo</th>\n",
       "      <th>tiprel_1mes</th>\n",
       "      <th>indresi</th>\n",
       "      <th>indext</th>\n",
       "      <th>conyuemp</th>\n",
       "      <th>canal_entrada</th>\n",
       "      <th>indfall</th>\n",
       "      <th>tipodom</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_prev</th>\n",
       "      <th>ind_plan_fin_ult1_prev</th>\n",
       "      <th>ind_pres_fin_ult1_prev</th>\n",
       "      <th>ind_reca_fin_ult1_prev</th>\n",
       "      <th>ind_tjcr_fin_ult1_prev</th>\n",
       "      <th>ind_valo_fin_ult1_prev</th>\n",
       "      <th>ind_viv_fin_ult1_prev</th>\n",
       "      <th>ind_nomina_ult1_prev</th>\n",
       "      <th>ind_nom_pens_ult1_prev</th>\n",
       "      <th>ind_recibo_ult1_prev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10394531</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10394532</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10394533</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10394534</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10394535</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11091065</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11091066</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11091067</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11091068</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11091069</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>696539 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ind_empleado  pais_residencia  sexo  tiprel_1mes  indresi  indext  \\\n",
       "10394531             0                0     0            0        0       0   \n",
       "10394532             0                0     1            0        0       0   \n",
       "10394533             0                0     1            0        0       0   \n",
       "10394534             0                0     0            0        0       0   \n",
       "10394535             0                0     1            0        0       0   \n",
       "...                ...              ...   ...          ...      ...     ...   \n",
       "11091065             0                0     1            1        0       0   \n",
       "11091066             0                0     1            1        0       0   \n",
       "11091067             0                0     0            0        0       0   \n",
       "11091068             0                0     0            1        0       0   \n",
       "11091069             0                0     0          -99        0       0   \n",
       "\n",
       "          conyuemp  canal_entrada  indfall  tipodom  ...  \\\n",
       "10394531       -99            101        0        0  ...   \n",
       "10394532       -99             45        0        0  ...   \n",
       "10394533       -99            101        0        0  ...   \n",
       "10394534       -99              5        0        0  ...   \n",
       "10394535       -99              5        0        0  ...   \n",
       "...            ...            ...      ...      ...  ...   \n",
       "11091065       -99              1        0        0  ...   \n",
       "11091066       -99              1        0        0  ...   \n",
       "11091067       -99              1        0        0  ...   \n",
       "11091068       -99              1        0        0  ...   \n",
       "11091069       -99            -99        0        0  ...   \n",
       "\n",
       "          ind_hip_fin_ult1_prev  ind_plan_fin_ult1_prev  \\\n",
       "10394531                    0.0                     0.0   \n",
       "10394532                    0.0                     0.0   \n",
       "10394533                    0.0                     0.0   \n",
       "10394534                    0.0                     0.0   \n",
       "10394535                    0.0                     0.0   \n",
       "...                         ...                     ...   \n",
       "11091065                    0.0                     0.0   \n",
       "11091066                    0.0                     0.0   \n",
       "11091067                    0.0                     0.0   \n",
       "11091068                    0.0                     0.0   \n",
       "11091069                    0.0                     0.0   \n",
       "\n",
       "          ind_pres_fin_ult1_prev  ind_reca_fin_ult1_prev  \\\n",
       "10394531                     0.0                     0.0   \n",
       "10394532                     0.0                     0.0   \n",
       "10394533                     0.0                     0.0   \n",
       "10394534                     0.0                     0.0   \n",
       "10394535                     0.0                     0.0   \n",
       "...                          ...                     ...   \n",
       "11091065                     0.0                     0.0   \n",
       "11091066                     0.0                     0.0   \n",
       "11091067                     0.0                     0.0   \n",
       "11091068                     0.0                     0.0   \n",
       "11091069                     0.0                     0.0   \n",
       "\n",
       "          ind_tjcr_fin_ult1_prev  ind_valo_fin_ult1_prev  \\\n",
       "10394531                     0.0                     0.0   \n",
       "10394532                     0.0                     0.0   \n",
       "10394533                     1.0                     0.0   \n",
       "10394534                     0.0                     0.0   \n",
       "10394535                     1.0                     0.0   \n",
       "...                          ...                     ...   \n",
       "11091065                     0.0                     0.0   \n",
       "11091066                     0.0                     0.0   \n",
       "11091067                     0.0                     0.0   \n",
       "11091068                     0.0                     0.0   \n",
       "11091069                     0.0                     0.0   \n",
       "\n",
       "          ind_viv_fin_ult1_prev  ind_nomina_ult1_prev  ind_nom_pens_ult1_prev  \\\n",
       "10394531                    0.0                   0.0                     0.0   \n",
       "10394532                    0.0                   0.0                     0.0   \n",
       "10394533                    0.0                   1.0                     1.0   \n",
       "10394534                    0.0                   0.0                     0.0   \n",
       "10394535                    0.0                   0.0                     0.0   \n",
       "...                         ...                   ...                     ...   \n",
       "11091065                    0.0                   0.0                     0.0   \n",
       "11091066                    0.0                   0.0                     0.0   \n",
       "11091067                    0.0                   0.0                     0.0   \n",
       "11091068                    0.0                   0.0                     0.0   \n",
       "11091069                    0.0                   0.0                     0.0   \n",
       "\n",
       "          ind_recibo_ult1_prev  \n",
       "10394531                   0.0  \n",
       "10394532                   0.0  \n",
       "10394533                   1.0  \n",
       "10394534                   0.0  \n",
       "10394535                   0.0  \n",
       "...                        ...  \n",
       "11091065                   0.0  \n",
       "11091066                   0.0  \n",
       "11091067                   0.0  \n",
       "11091068                   0.0  \n",
       "11091069                   0.0  \n",
       "\n",
       "[696539 rows x 70 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vld[features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAP@7 rating scale fluctuates according to the data with the highest score. The highest score of MAP@7 that can be obtained from the validation data of this baseline model is 0.042633. Our baseline model is"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = XY[features].values\n",
    "Y_all = XY['y'].values\n",
    "dall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\n",
    "watch_list = [(dall, 'train')]\n",
    "\n",
    "# Increase Number of tress propertional to the increased size of data\n",
    "best_ntree_limit = int(best_ntree_limit * (len(XY_trn) + len(XY_vld)) / len(XY_trn))\n",
    "\n",
    "# retrain the xgb model\n",
    "model = xgb.train(param, dall, num_boost_round=best_ntree_limit, evals=watch_list)\n",
    "\n",
    "# Check the feature importance to see if feature that are assumed to be important listed on top\n",
    "print(\"Feature importance:\")\n",
    "for kv in sorted([(k,v) for k,v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
    "    print(kv)\n",
    "\n",
    "# calculate test data prediction result\n",
    "X_tst = tst[features].values\n",
    "dtst = xgb.DMatrix(X_tst, feature_names=features)\n",
    "preds_tst = model.predict(dtst, ntree_limit=best_ntree_limit)\n",
    "ncodpers_tst = tst['ncodpers'].values\n",
    "preds_tst = preds_tst - tst[[prod + '_prev' for prod in prods]].values\n",
    "\n",
    "# submission file creation\n",
    "submit_file = open('./model/xgb.baseline.2023-03-24.csv', 'w')\n",
    "submit_file.write('ncodpers,added_products\\n')\n",
    "\n",
    "numRows = 0\n",
    "for ncodper, pred in zip(ncodpers_tst, preds_tst):\n",
    "    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    y_prods = [p for y,p,ip in y_prods]\n",
    "    submit_file.write('{},{}\\n'.format(int(ncodper), ' '.join(y_prods)))\n",
    "    numRows += 1\n",
    "print(numRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "santander",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
