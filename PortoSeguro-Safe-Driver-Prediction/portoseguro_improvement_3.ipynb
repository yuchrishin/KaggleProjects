{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle - Porte Seguro Safe Driver Prediction - Perfomance Improvement 3\n",
    "**Author: Chris Shin**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods are techniques in machine learning where multiple models are trained and combined to improve the overall performance and robustness of the prediction. The idea behind ensemble methods is to create a diverse set of models that are capable of capturing different aspects of the data, and then combine their predictions to produce a more accurate and reliable result.\n",
    "\n",
    "Ensemble methods can be used in both supervised and unsupervised learning tasks. In supervised learning, ensemble methods are typically used for classification and regression tasks, while in unsupervised learning, they are used for clustering and dimensionality reduction tasks.\n",
    "\n",
    "There are several reasons why ensemble methods can be effective:\n",
    "\n",
    "1. Reduction of overfitting: Ensemble methods can help reduce overfitting, which is a common problem in machine learning where a model performs well on the training data but poorly on the test data. By combining multiple models, ensemble methods can reduce the risk of overfitting by averaging out errors and reducing variance.\n",
    "\n",
    "2. Improved accuracy: Ensemble methods can improve the accuracy of predictions by combining the strengths of multiple models. This can result in more robust and accurate predictions, especially when dealing with complex and noisy datasets.\n",
    "\n",
    "3. Increased stability: Ensemble methods can be more stable than single models because they are less sensitive to variations in the data. This can be especially important in real-world applications where the data is constantly changing or noisy.\n",
    "\n",
    "4. Flexibility: Ensemble methods are flexible and can be used with a wide range of machine learning algorithms, including decision trees, neural networks, and support vector machines.\n",
    "\n",
    "Overall, ensemble methods can be a powerful tool in machine learning and can help improve the accuracy, stability, and robustness of predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods are not always better than single models. It depends on the specific problem and data. In some cases, a well-tuned single model may outperform an ensemble of models. However, ensembles can often provide better results by combining the strengths of multiple models and reducing the weaknesses of any single model. Ensembles can also be more robust and less prone to overfitting than single models. In general, it is a good idea to experiment with both single models and ensembles and choose the approach that gives the best performance for the specific task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "train = pd.read_csv('./data/train.csv', index_col='id')\n",
    "test = pd.read_csv('./data/test.csv', index_col='id')\n",
    "submission = pd.read_csv('./data/sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test], ignore_index=True)\n",
    "all_data = all_data.drop('target', axis=1) \n",
    "\n",
    "all_features = all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature] \n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['num_missing'] = (all_data==-1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_features = [feature for feature in all_features\n",
    "                      if ('cat' not in feature and 'calc' not in feature)] \n",
    "remaining_features.append('num_missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_features = [feature for feature in all_features if 'ind' in feature]\n",
    "\n",
    "is_first_feature = True\n",
    "for ind_feature in ind_features:\n",
    "    if is_first_feature:\n",
    "        all_data['mix_ind'] = all_data[ind_feature].astype(str) + '_'\n",
    "        is_first_feature = False\n",
    "    else:\n",
    "        all_data['mix_ind'] += all_data[ind_feature].astype(str) + '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_count_features = []\n",
    "for feature in cat_features+['mix_ind']:\n",
    "    val_counts_dict = all_data[feature].value_counts().to_dict()\n",
    "    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x: \n",
    "                                                           val_counts_dict[x])\n",
    "    cat_count_features.append(f'{feature}_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "drop_features = ['ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin', \n",
    "                 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_14']\n",
    "\n",
    "all_data_remaining = all_data[remaining_features+cat_count_features].drop(drop_features, axis=1)\n",
    "\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining),\n",
    "                               encoded_cat_matrix],\n",
    "                              format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train)\n",
    "\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train:]\n",
    "\n",
    "y = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_gini(y_true, y_pred):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    n_samples = y_true.shape[0]\n",
    "    L_mid = np.linspace(1 / n_samples, 1, n_samples)\n",
    "\n",
    "    pred_order = y_true[y_pred.argsort()]\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order) \n",
    "    G_pred = np.sum(L_mid - L_pred) \n",
    "\n",
    "    true_order = y_true[y_true.argsort()]\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order) \n",
    "    G_true = np.sum(L_mid - L_true)\n",
    "\n",
    "    return G_pred / G_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini for LGB\n",
    "def gini_lgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', eval_gini(labels, preds), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini for XGB\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', eval_gini(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_lgb = {\n",
    "    'bagging_fraction': 0.6213108174593661,\n",
    "    'feature_fraction': 0.608712929970154,\n",
    "    'lambda_l1': 0.7040436794880651,\n",
    "    'lambda_l2': 0.9832619845547939,\n",
    "    'min_child_samples': 9,\n",
    "    'min_child_weight': 36.10036444740457,\n",
    "    'num_leaves': 40,\n",
    "    'objective': 'binary',\n",
    "    'learning_rate': 0.005,\n",
    "    'bagging_freq': 1,\n",
    "    'force_row_wise': True,\n",
    "    'random_state': 1991\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## Fold 1 / fold 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yuchie\\Desktop\\DataScience\\kaggle_env\\Lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\Yuchie\\Desktop\\DataScience\\kaggle_env\\Lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154239\tvalid_0's gini: 0.270944\n",
      "[200]\tvalid_0's binary_logloss: 0.153176\tvalid_0's gini: 0.275764\n",
      "[300]\tvalid_0's binary_logloss: 0.152584\tvalid_0's gini: 0.279501\n",
      "[400]\tvalid_0's binary_logloss: 0.152222\tvalid_0's gini: 0.282893\n",
      "[500]\tvalid_0's binary_logloss: 0.151986\tvalid_0's gini: 0.286058\n",
      "[600]\tvalid_0's binary_logloss: 0.151824\tvalid_0's gini: 0.288805\n",
      "[700]\tvalid_0's binary_logloss: 0.151712\tvalid_0's gini: 0.290719\n",
      "[800]\tvalid_0's binary_logloss: 0.151622\tvalid_0's gini: 0.292581\n",
      "[900]\tvalid_0's binary_logloss: 0.151552\tvalid_0's gini: 0.294212\n",
      "[1000]\tvalid_0's binary_logloss: 0.151505\tvalid_0's gini: 0.295204\n",
      "[1100]\tvalid_0's binary_logloss: 0.151471\tvalid_0's gini: 0.295909\n",
      "[1200]\tvalid_0's binary_logloss: 0.151438\tvalid_0's gini: 0.296721\n",
      "[1300]\tvalid_0's binary_logloss: 0.151414\tvalid_0's gini: 0.297335\n",
      "[1400]\tvalid_0's binary_logloss: 0.151402\tvalid_0's gini: 0.297569\n",
      "[1500]\tvalid_0's binary_logloss: 0.15139\tvalid_0's gini: 0.297881\n",
      "[1600]\tvalid_0's binary_logloss: 0.151382\tvalid_0's gini: 0.298033\n",
      "[1700]\tvalid_0's binary_logloss: 0.151376\tvalid_0's gini: 0.298238\n",
      "[1800]\tvalid_0's binary_logloss: 0.151372\tvalid_0's gini: 0.298342\n",
      "[1900]\tvalid_0's binary_logloss: 0.151369\tvalid_0's gini: 0.298371\n",
      "[2000]\tvalid_0's binary_logloss: 0.151371\tvalid_0's gini: 0.298222\n",
      "[2100]\tvalid_0's binary_logloss: 0.151362\tvalid_0's gini: 0.298463\n",
      "[2200]\tvalid_0's binary_logloss: 0.151359\tvalid_0's gini: 0.298466\n",
      "[2300]\tvalid_0's binary_logloss: 0.151362\tvalid_0's gini: 0.298415\n",
      "[2400]\tvalid_0's binary_logloss: 0.151359\tvalid_0's gini: 0.298569\n",
      "[2500]\tvalid_0's binary_logloss: 0.151361\tvalid_0's gini: 0.298542\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2458]\tvalid_0's binary_logloss: 0.151355\tvalid_0's gini: 0.29865\n",
      "Fold 1 gini coefficient : 0.2986504843987991\n",
      "\n",
      "######################################## Fold 2 / fold 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1560\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154347\tvalid_0's gini: 0.258575\n",
      "[200]\tvalid_0's binary_logloss: 0.153338\tvalid_0's gini: 0.263768\n",
      "[300]\tvalid_0's binary_logloss: 0.152804\tvalid_0's gini: 0.267635\n",
      "[400]\tvalid_0's binary_logloss: 0.152483\tvalid_0's gini: 0.271009\n",
      "[500]\tvalid_0's binary_logloss: 0.152299\tvalid_0's gini: 0.27324\n",
      "[600]\tvalid_0's binary_logloss: 0.152157\tvalid_0's gini: 0.275756\n",
      "[700]\tvalid_0's binary_logloss: 0.15206\tvalid_0's gini: 0.277655\n",
      "[800]\tvalid_0's binary_logloss: 0.151986\tvalid_0's gini: 0.279371\n",
      "[900]\tvalid_0's binary_logloss: 0.151942\tvalid_0's gini: 0.280359\n",
      "[1000]\tvalid_0's binary_logloss: 0.151898\tvalid_0's gini: 0.281475\n",
      "[1100]\tvalid_0's binary_logloss: 0.15186\tvalid_0's gini: 0.282482\n",
      "[1200]\tvalid_0's binary_logloss: 0.151835\tvalid_0's gini: 0.283198\n",
      "[1300]\tvalid_0's binary_logloss: 0.15181\tvalid_0's gini: 0.283848\n",
      "[1400]\tvalid_0's binary_logloss: 0.151796\tvalid_0's gini: 0.284221\n",
      "[1500]\tvalid_0's binary_logloss: 0.151781\tvalid_0's gini: 0.284645\n",
      "[1600]\tvalid_0's binary_logloss: 0.15177\tvalid_0's gini: 0.284943\n",
      "[1700]\tvalid_0's binary_logloss: 0.151761\tvalid_0's gini: 0.285129\n",
      "[1800]\tvalid_0's binary_logloss: 0.151755\tvalid_0's gini: 0.28522\n",
      "[1900]\tvalid_0's binary_logloss: 0.151752\tvalid_0's gini: 0.285325\n",
      "[2000]\tvalid_0's binary_logloss: 0.151749\tvalid_0's gini: 0.285504\n",
      "[2100]\tvalid_0's binary_logloss: 0.151748\tvalid_0's gini: 0.285633\n",
      "[2200]\tvalid_0's binary_logloss: 0.151744\tvalid_0's gini: 0.285711\n",
      "[2300]\tvalid_0's binary_logloss: 0.15174\tvalid_0's gini: 0.285853\n",
      "[2400]\tvalid_0's binary_logloss: 0.15174\tvalid_0's gini: 0.28594\n",
      "[2500]\tvalid_0's binary_logloss: 0.151745\tvalid_0's gini: 0.285916\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2334]\tvalid_0's binary_logloss: 0.151736\tvalid_0's gini: 0.285929\n",
      "Fold 2 gini coefficient : 0.2859292916021393\n",
      "\n",
      "######################################## Fold 3 / fold 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.15424\tvalid_0's gini: 0.263985\n",
      "[200]\tvalid_0's binary_logloss: 0.153171\tvalid_0's gini: 0.268713\n",
      "[300]\tvalid_0's binary_logloss: 0.152574\tvalid_0's gini: 0.272773\n",
      "[400]\tvalid_0's binary_logloss: 0.152223\tvalid_0's gini: 0.275785\n",
      "[500]\tvalid_0's binary_logloss: 0.152001\tvalid_0's gini: 0.278098\n",
      "[600]\tvalid_0's binary_logloss: 0.151847\tvalid_0's gini: 0.280206\n",
      "[700]\tvalid_0's binary_logloss: 0.151748\tvalid_0's gini: 0.281603\n",
      "[800]\tvalid_0's binary_logloss: 0.151682\tvalid_0's gini: 0.282672\n",
      "[900]\tvalid_0's binary_logloss: 0.151637\tvalid_0's gini: 0.283423\n",
      "[1000]\tvalid_0's binary_logloss: 0.151608\tvalid_0's gini: 0.283963\n",
      "[1100]\tvalid_0's binary_logloss: 0.151589\tvalid_0's gini: 0.284105\n",
      "[1200]\tvalid_0's binary_logloss: 0.151574\tvalid_0's gini: 0.284387\n",
      "[1300]\tvalid_0's binary_logloss: 0.151575\tvalid_0's gini: 0.284318\n",
      "[1400]\tvalid_0's binary_logloss: 0.151572\tvalid_0's gini: 0.284372\n",
      "[1500]\tvalid_0's binary_logloss: 0.151569\tvalid_0's gini: 0.284466\n",
      "[1600]\tvalid_0's binary_logloss: 0.151574\tvalid_0's gini: 0.284435\n",
      "[1700]\tvalid_0's binary_logloss: 0.151579\tvalid_0's gini: 0.284362\n",
      "Early stopping, best iteration is:\n",
      "[1478]\tvalid_0's binary_logloss: 0.151568\tvalid_0's gini: 0.284492\n",
      "Fold 3 gini coefficient : 0.2844916047790675\n",
      "\n",
      "######################################## Fold 4 / fold 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154327\tvalid_0's gini: 0.256916\n",
      "[200]\tvalid_0's binary_logloss: 0.15331\tvalid_0's gini: 0.261871\n",
      "[300]\tvalid_0's binary_logloss: 0.152761\tvalid_0's gini: 0.265441\n",
      "[400]\tvalid_0's binary_logloss: 0.152441\tvalid_0's gini: 0.268613\n",
      "[500]\tvalid_0's binary_logloss: 0.152245\tvalid_0's gini: 0.271168\n",
      "[600]\tvalid_0's binary_logloss: 0.152098\tvalid_0's gini: 0.273746\n",
      "[700]\tvalid_0's binary_logloss: 0.152012\tvalid_0's gini: 0.275192\n",
      "[800]\tvalid_0's binary_logloss: 0.151952\tvalid_0's gini: 0.276278\n",
      "[900]\tvalid_0's binary_logloss: 0.151911\tvalid_0's gini: 0.277039\n",
      "[1000]\tvalid_0's binary_logloss: 0.151871\tvalid_0's gini: 0.277996\n",
      "[1100]\tvalid_0's binary_logloss: 0.151844\tvalid_0's gini: 0.278535\n",
      "[1200]\tvalid_0's binary_logloss: 0.151827\tvalid_0's gini: 0.279055\n",
      "[1300]\tvalid_0's binary_logloss: 0.151817\tvalid_0's gini: 0.27936\n",
      "[1400]\tvalid_0's binary_logloss: 0.151799\tvalid_0's gini: 0.279872\n",
      "[1500]\tvalid_0's binary_logloss: 0.151797\tvalid_0's gini: 0.280053\n",
      "[1600]\tvalid_0's binary_logloss: 0.151792\tvalid_0's gini: 0.280148\n",
      "[1700]\tvalid_0's binary_logloss: 0.151794\tvalid_0's gini: 0.280162\n",
      "[1800]\tvalid_0's binary_logloss: 0.151793\tvalid_0's gini: 0.280319\n",
      "[1900]\tvalid_0's binary_logloss: 0.151795\tvalid_0's gini: 0.280422\n",
      "[2000]\tvalid_0's binary_logloss: 0.151797\tvalid_0's gini: 0.280419\n",
      "[2100]\tvalid_0's binary_logloss: 0.151799\tvalid_0's gini: 0.280516\n",
      "Early stopping, best iteration is:\n",
      "[1852]\tvalid_0's binary_logloss: 0.15179\tvalid_0's gini: 0.280514\n",
      "Fold 4 gini coefficient : 0.2805136229288192\n",
      "\n",
      "######################################## Fold 5 / fold 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.15439\tvalid_0's gini: 0.26681\n",
      "[200]\tvalid_0's binary_logloss: 0.15338\tvalid_0's gini: 0.272186\n",
      "[300]\tvalid_0's binary_logloss: 0.152821\tvalid_0's gini: 0.275897\n",
      "[400]\tvalid_0's binary_logloss: 0.1525\tvalid_0's gini: 0.278734\n",
      "[500]\tvalid_0's binary_logloss: 0.152277\tvalid_0's gini: 0.282151\n",
      "[600]\tvalid_0's binary_logloss: 0.15212\tvalid_0's gini: 0.285039\n",
      "[700]\tvalid_0's binary_logloss: 0.152009\tvalid_0's gini: 0.287435\n",
      "[800]\tvalid_0's binary_logloss: 0.15192\tvalid_0's gini: 0.289549\n",
      "[900]\tvalid_0's binary_logloss: 0.151862\tvalid_0's gini: 0.290886\n",
      "[1000]\tvalid_0's binary_logloss: 0.151819\tvalid_0's gini: 0.291935\n",
      "[1100]\tvalid_0's binary_logloss: 0.151782\tvalid_0's gini: 0.292972\n",
      "[1200]\tvalid_0's binary_logloss: 0.151752\tvalid_0's gini: 0.293784\n",
      "[1300]\tvalid_0's binary_logloss: 0.151732\tvalid_0's gini: 0.294315\n",
      "[1400]\tvalid_0's binary_logloss: 0.151724\tvalid_0's gini: 0.294475\n",
      "[1500]\tvalid_0's binary_logloss: 0.151713\tvalid_0's gini: 0.294786\n",
      "[1600]\tvalid_0's binary_logloss: 0.1517\tvalid_0's gini: 0.295146\n",
      "[1700]\tvalid_0's binary_logloss: 0.151694\tvalid_0's gini: 0.295268\n",
      "[1800]\tvalid_0's binary_logloss: 0.151695\tvalid_0's gini: 0.295212\n",
      "[1900]\tvalid_0's binary_logloss: 0.151689\tvalid_0's gini: 0.295454\n",
      "[2000]\tvalid_0's binary_logloss: 0.151693\tvalid_0's gini: 0.2954\n",
      "[2100]\tvalid_0's binary_logloss: 0.151694\tvalid_0's gini: 0.295427\n",
      "[2200]\tvalid_0's binary_logloss: 0.151692\tvalid_0's gini: 0.295538\n",
      "[2300]\tvalid_0's binary_logloss: 0.151699\tvalid_0's gini: 0.295411\n",
      "Early stopping, best iteration is:\n",
      "[2045]\tvalid_0's binary_logloss: 0.151689\tvalid_0's gini: 0.295553\n",
      "Fold 5 gini coefficient : 0.29555250456072807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "oof_val_preds_lgb = np.zeros(X.shape[0]) \n",
    "oof_test_preds_lgb = np.zeros(X_test.shape[0]) \n",
    "\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    print('#'*40, f'Fold {idx+1} / fold {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train, y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid)\n",
    "                          \n",
    "    lgb_model = lgb.train(params=max_params_lgb,\n",
    "                          train_set=dtrain,\n",
    "                          num_boost_round=2500,\n",
    "                          valid_sets=dvalid,\n",
    "                          feval=gini_lgb,\n",
    "                          early_stopping_rounds=300,\n",
    "                          verbose_eval=100)\n",
    "    \n",
    "    oof_test_preds_lgb += lgb_model.predict(X_test)/folds.n_splits\n",
    "    \n",
    "    oof_val_preds_lgb[valid_idx] += lgb_model.predict(X_valid)\n",
    "    \n",
    "    gini_score = eval_gini(y_valid, oof_val_preds_lgb[valid_idx])\n",
    "    print(f'Fold {idx+1} gini coefficient : {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_xgb = {\n",
    "    'colsample_bytree': 0.8843124587484356,\n",
    "    'gamma': 10.452246227672624,\n",
    "    'max_depth': 7,\n",
    "    'min_child_weight': 6.494091293383359,\n",
    "    'reg_alpha': 8.551838810159788,\n",
    "    'reg_lambda': 1.3814765995549108,\n",
    "    'scale_pos_weight': 1.423280772455086,\n",
    "    'subsample': 0.7001630536555632,\n",
    "    'objective': 'binary:logistic',\n",
    "    'learning_rate': 0.02,\n",
    "    'random_state': 1991\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## Fold 1 / fold 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yuchie\\Desktop\\DataScience\\kaggle_env\\Lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalid-logloss:0.67665\tvalid-gini:0.15993\n",
      "[100]\tvalid-logloss:0.19089\tvalid-gini:0.24884\n",
      "[200]\tvalid-logloss:0.15780\tvalid-gini:0.27713\n",
      "[300]\tvalid-logloss:0.15458\tvalid-gini:0.28754\n",
      "[400]\tvalid-logloss:0.15405\tvalid-gini:0.29224\n",
      "[500]\tvalid-logloss:0.15390\tvalid-gini:0.29505\n",
      "[600]\tvalid-logloss:0.15385\tvalid-gini:0.29615\n",
      "[700]\tvalid-logloss:0.15380\tvalid-gini:0.29718\n",
      "[800]\tvalid-logloss:0.15379\tvalid-gini:0.29778\n",
      "[900]\tvalid-logloss:0.15376\tvalid-gini:0.29806\n",
      "[1000]\tvalid-logloss:0.15377\tvalid-gini:0.29791\n",
      "[1100]\tvalid-logloss:0.15375\tvalid-gini:0.29822\n",
      "[1200]\tvalid-logloss:0.15375\tvalid-gini:0.29811\n",
      "[1300]\tvalid-logloss:0.15373\tvalid-gini:0.29846\n",
      "[1400]\tvalid-logloss:0.15374\tvalid-gini:0.29845\n",
      "[1500]\tvalid-logloss:0.15372\tvalid-gini:0.29873\n",
      "[1600]\tvalid-logloss:0.15374\tvalid-gini:0.29847\n",
      "[1693]\tvalid-logloss:0.15373\tvalid-gini:0.29848\n",
      "######################################## Fold 2 / fold 5 ########################################\n",
      "[0]\tvalid-logloss:0.67666\tvalid-gini:0.12533\n",
      "[100]\tvalid-logloss:0.19104\tvalid-gini:0.23388\n",
      "[200]\tvalid-logloss:0.15807\tvalid-gini:0.26288\n",
      "[300]\tvalid-logloss:0.15496\tvalid-gini:0.27258\n",
      "[400]\tvalid-logloss:0.15449\tvalid-gini:0.27692\n",
      "[500]\tvalid-logloss:0.15436\tvalid-gini:0.27907\n",
      "[600]\tvalid-logloss:0.15430\tvalid-gini:0.28082\n",
      "[700]\tvalid-logloss:0.15428\tvalid-gini:0.28165\n",
      "[800]\tvalid-logloss:0.15426\tvalid-gini:0.28260\n",
      "[900]\tvalid-logloss:0.15423\tvalid-gini:0.28305\n",
      "[1000]\tvalid-logloss:0.15422\tvalid-gini:0.28312\n",
      "[1100]\tvalid-logloss:0.15422\tvalid-gini:0.28341\n",
      "[1200]\tvalid-logloss:0.15421\tvalid-gini:0.28354\n",
      "[1300]\tvalid-logloss:0.15421\tvalid-gini:0.28362\n",
      "[1400]\tvalid-logloss:0.15418\tvalid-gini:0.28399\n",
      "[1500]\tvalid-logloss:0.15420\tvalid-gini:0.28411\n",
      "[1600]\tvalid-logloss:0.15419\tvalid-gini:0.28408\n",
      "[1700]\tvalid-logloss:0.15417\tvalid-gini:0.28427\n",
      "[1800]\tvalid-logloss:0.15420\tvalid-gini:0.28418\n",
      "[1900]\tvalid-logloss:0.15418\tvalid-gini:0.28415\n",
      "[1923]\tvalid-logloss:0.15418\tvalid-gini:0.28429\n",
      "######################################## Fold 3 / fold 5 ########################################\n",
      "[0]\tvalid-logloss:0.67665\tvalid-gini:0.15482\n",
      "[100]\tvalid-logloss:0.19091\tvalid-gini:0.24517\n",
      "[200]\tvalid-logloss:0.15779\tvalid-gini:0.27099\n",
      "[300]\tvalid-logloss:0.15462\tvalid-gini:0.27900\n",
      "[400]\tvalid-logloss:0.15415\tvalid-gini:0.28187\n",
      "[500]\tvalid-logloss:0.15407\tvalid-gini:0.28242\n",
      "[600]\tvalid-logloss:0.15403\tvalid-gini:0.28324\n",
      "[700]\tvalid-logloss:0.15402\tvalid-gini:0.28313\n",
      "[800]\tvalid-logloss:0.15400\tvalid-gini:0.28328\n",
      "[824]\tvalid-logloss:0.15400\tvalid-gini:0.28343\n",
      "######################################## Fold 4 / fold 5 ########################################\n",
      "[0]\tvalid-logloss:0.67664\tvalid-gini:0.15196\n",
      "[100]\tvalid-logloss:0.19084\tvalid-gini:0.23516\n",
      "[200]\tvalid-logloss:0.15790\tvalid-gini:0.26402\n",
      "[300]\tvalid-logloss:0.15480\tvalid-gini:0.27213\n",
      "[400]\tvalid-logloss:0.15435\tvalid-gini:0.27499\n",
      "[500]\tvalid-logloss:0.15425\tvalid-gini:0.27653\n",
      "[600]\tvalid-logloss:0.15420\tvalid-gini:0.27808\n",
      "[700]\tvalid-logloss:0.15417\tvalid-gini:0.27828\n",
      "[800]\tvalid-logloss:0.15416\tvalid-gini:0.27881\n",
      "[900]\tvalid-logloss:0.15414\tvalid-gini:0.27924\n",
      "[1000]\tvalid-logloss:0.15413\tvalid-gini:0.27937\n",
      "[1100]\tvalid-logloss:0.15415\tvalid-gini:0.27941\n",
      "[1200]\tvalid-logloss:0.15414\tvalid-gini:0.27943\n",
      "[1264]\tvalid-logloss:0.15415\tvalid-gini:0.27930\n",
      "######################################## Fold 5 / fold 5 ########################################\n",
      "[0]\tvalid-logloss:0.67666\tvalid-gini:0.14131\n",
      "[100]\tvalid-logloss:0.19093\tvalid-gini:0.24531\n",
      "[200]\tvalid-logloss:0.15803\tvalid-gini:0.27194\n",
      "[300]\tvalid-logloss:0.15488\tvalid-gini:0.28193\n",
      "[400]\tvalid-logloss:0.15437\tvalid-gini:0.28695\n",
      "[500]\tvalid-logloss:0.15423\tvalid-gini:0.28956\n",
      "[600]\tvalid-logloss:0.15416\tvalid-gini:0.29171\n",
      "[700]\tvalid-logloss:0.15409\tvalid-gini:0.29339\n",
      "[800]\tvalid-logloss:0.15407\tvalid-gini:0.29451\n",
      "[900]\tvalid-logloss:0.15404\tvalid-gini:0.29495\n",
      "[1000]\tvalid-logloss:0.15402\tvalid-gini:0.29534\n",
      "[1100]\tvalid-logloss:0.15402\tvalid-gini:0.29584\n",
      "[1200]\tvalid-logloss:0.15400\tvalid-gini:0.29609\n",
      "[1300]\tvalid-logloss:0.15397\tvalid-gini:0.29645\n",
      "[1400]\tvalid-logloss:0.15397\tvalid-gini:0.29667\n",
      "[1500]\tvalid-logloss:0.15396\tvalid-gini:0.29690\n",
      "[1600]\tvalid-logloss:0.15395\tvalid-gini:0.29731\n",
      "[1700]\tvalid-logloss:0.15393\tvalid-gini:0.29741\n",
      "[1800]\tvalid-logloss:0.15396\tvalid-gini:0.29715\n",
      "[1848]\tvalid-logloss:0.15396\tvalid-gini:0.29720\n",
      "Fold 5 gini coefficient : 0.2974367097838099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "oof_val_preds_xgb = np.zeros(X.shape[0]) \n",
    "oof_test_preds_xgb = np.zeros(X_test.shape[0]) \n",
    "\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    print('#'*40, f'Fold {idx+1} / fold {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    xgb_model = xgb.train(params=max_params_xgb, \n",
    "                          dtrain=dtrain,\n",
    "                          num_boost_round=2000,\n",
    "                          evals=[(dvalid, 'valid')],\n",
    "                          maximize=True,\n",
    "                          feval=gini_xgb,\n",
    "                          early_stopping_rounds=200,\n",
    "                          verbose_eval=100)\n",
    "\n",
    "    best_iter = xgb_model.best_iteration\n",
    "\n",
    "    oof_test_preds_xgb += xgb_model.predict(dtest,\n",
    "                                            iteration_range=(0, best_iter))/folds.n_splits\n",
    "    \n",
    "    oof_val_preds_xgb[valid_idx] += xgb_model.predict(dvalid, \n",
    "                                                      iteration_range=(0, best_iter))\n",
    "    \n",
    "    gini_score = eval_gini(y_valid, oof_val_preds_xgb[valid_idx])\n",
    "print(f'Fold {idx+1} gini coefficient : {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM OOF  OOF validation prediction gini coefficient: 0.2889651000887542\n"
     ]
    }
   ],
   "source": [
    "print('LightGBM OOF  OOF validation prediction gini coefficient:', eval_gini(y, oof_val_preds_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost OOF  OOF validation prediction gini coefficient: 0.28863101798154267\n"
     ]
    }
   ],
   "source": [
    "print('XGBoost OOF  OOF validation prediction gini coefficient:', eval_gini(y, oof_val_preds_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_test_preds = oof_test_preds_lgb * 0.5 + oof_test_preds_xgb * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
