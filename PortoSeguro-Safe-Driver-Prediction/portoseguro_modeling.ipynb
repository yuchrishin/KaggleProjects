{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle - Porte Seguro Safe Driver Prediction - Baseline Model\n",
    "**Author: Chris Shin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('./data/train.csv', index_col='id')\n",
    "test = pd.read_csv('./data/test.csv', index_col='id')\n",
    "submission = pd.read_csv('./data/sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data but taking out target for data processing and feature engineering\n",
    "all_data = pd.concat([train, test], ignore_index=True)\n",
    "all_data.drop('target', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n",
       "       'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n",
       "       'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n",
       "       'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n",
       "       'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n",
       "       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n",
       "       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n",
       "       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n",
       "       'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n",
       "       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n",
       "       'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n",
       "       'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n",
       "       'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n",
       "       'ps_calc_20_bin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = all_data.columns\n",
    "all_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Feature One-Hot Encoding\n",
    "From EDA we decided to keep all categorical features. We will perform one hot encoding for all categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1488028x184 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 20832392 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])\n",
    "encoded_cat_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After encoding we have total 184 features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "Features to remove based on EDA:\n",
    "\n",
    "Considered weak features:\n",
    "- `ps_ind_10_bin`, `ps_ind_11_bin`, `ps_ind_12_bin`, `ps_ind_13_bin`, `ps_ind_14`\n",
    "\n",
    "High Correlation:\n",
    "- `ps_car_12` and `ps_car_14` -> `ps_car_14`\n",
    "- `ps_reg_02` and `ps_reg_03` -> `ps_reg_03`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = ['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin',\n",
    "                 'ps_ind_14', 'ps_car_14', 'ps_reg_03']\n",
    "\n",
    "remaining_features = [feature for feature in all_features\n",
    "                      if('cat' not in feature and\n",
    "                         'calc' not in feature and\n",
    "                         feature not in drop_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data[remaining_features]),\n",
    "                               encoded_cat_matrix],\n",
    "                              format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1488028x200 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 36140946 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_sprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train:]\n",
    "\n",
    "y = train['target'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Before we move into model training, we need to define the evaluation metrics first. In this Kaggle project, scoring metrics used to evaluate the submission is Normalized Gini Coefficient. The Normalized Gini coefficient is how far away we are with our sorted actual values from a random state measured in number of swaps. Since it is normalized, the range of coefficient is normalized into value of 0 to 1. Thus when it close to 0, performance is bad, and when it close to 1, the performance is good.\n",
    "\n",
    "<img src=\"./image/Gini_Index.png\" alt=\"Gini Coefficient Plot\" />\n",
    "source:https://www.nature.com/articles/s41598-019-54288-7\n",
    "\n",
    "In terms of machine learning, normalized gini coefficient is\n",
    "\n",
    "Normalized Gini Coefficient = Gini Coefficient of Predicted values / Gini Coefficient of Perfect Predictions\n",
    "\n",
    "Gini Coefficient of Predicted values = Gini Coefficient calculated with predicted values and actual values\n",
    "\n",
    "Gini Coefficient of Perfect Predictions = Gini Coefficient calculated with actual values and actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_gini(y_true, y_pred):\n",
    "    # checking if the preds and actuals have same size\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    # number of data\n",
    "    n_samples = y_true.shape[0]\n",
    "    # Get diagonal data\n",
    "    L_mid = np.linspace(1 / n_samples, 1, n_samples)\n",
    "\n",
    "    # 1) Gini Coefficient on Predictions\n",
    "    pred_order = y_true[y_pred.argsort()] # y_pred 크기순으로 y_true 값 정렬\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선\n",
    "    G_pred = np.sum(L_mid - L_pred)       # 예측 값에 대한 지니계수\n",
    "\n",
    "    # 2) Gini Coefficient on Actual values (when predictions perfect)\n",
    "    true_order = y_true[y_true.argsort()] # y_true 크기순으로 y_true 값 정렬\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order) # 로렌츠 곡선\n",
    "    G_true = np.sum(L_mid - L_true)       # 예측이 완벽할 때 지니계수\n",
    "\n",
    "    # 정규화된 지니계수\n",
    "    return G_pred / G_true\n",
    "\n",
    "def gini(preds, dtrain):\n",
    "    # get dataset's target values -> actual values\n",
    "    labels = dtrain.get_label()\n",
    "    # return with name of evaluation metric, evaluation scores, and indication of whether scoring is good or bad.\n",
    "    # For now, we will set True as default as it is baseline model\n",
    "    return 'gini', eval_gini(labels, preds), True # 반환값"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "--------------------------\n",
    "# Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of Fold Prediction\n",
    "\n",
    "OOF prediction, out of fold prediction, is a method of predicting with test data for each fold while performing K-fold cross-validation. We perform K-fold cross-validation, and for each fold we follow below three actions.\n",
    "- 1) train the model with training data\n",
    "- 2) measure model performance with validation data\n",
    "- 3) predict the final target probability with test data\n",
    "\n",
    "It's not just a one-time prediction with a trained model. It is a method of making multiple predictions with the model for each fold and averaging them.\n",
    "\n",
    "**OOF Process Steps:**\n",
    "1. Divide the entire training data into K groups.\n",
    "2. One of the K groups is designated as validation data and the remaining K-1 groups as training data.\n",
    "3. Train the model with training data.\n",
    "4. Using the trained model, we predict target probabilities with validation data and target probabilities with full test data.\n",
    "5. Record the predicted probability obtained from the validation data and the predicted probability obtained from the test data.\n",
    "6. If you change the verification data to another group, repeat steps 2 to 5 a total of K times.\n",
    "7. Calculate the performance score by comparing the probability predicted by K groups of validation data with the actual target value of the training data. This number can be used to measure the performance of the model.\n",
    "8. Averages the K prediction probabilities from the test data. This value is the final predicted probability\n",
    "\n",
    "**Then What is the benefit of using OOF?**\n",
    "- It has the effect of preventing overfitting. Because of the K-fold method of evaluation, the training and validation data are different. Since performance is evaluated on data that has not been encountered during training, it is easy to gauge how well it will generalize to new data. This means that it is easy to respond to overfitting.\n",
    "- It has Ensemble effects which can improve model performance. An ensemble is a technique that combines the predictions of two or more models trained on the same training dataset. Instead of making one prediction with a single model, K models make K predictions and average them. Probabilities averaged over K predictions are usually more accurate than those predicted only once."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "We will use LightGMB for the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# We use stratifiedKFold since data is inbalance\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For baseline model simplified the parameter\n",
    "params = {'objective': 'binary',\n",
    "          'learning_rate': 0.01,\n",
    "          'force_row_wise': True,\n",
    "          'random_state': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A one-dimensional array containing the probability of predicting the \n",
    "# target value of the verification data with the model trained by the OOF method\n",
    "oof_val_preds = np.zeros(X.shape[0]) \n",
    "\n",
    "# A one-dimensional array containing the probability of predicting\n",
    "# the test data target value with a model trained by the OOF method\n",
    "oof_test_preds = np.zeros(X_test.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## fold 1 / fold 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 840\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 199\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153387\tvalid_0's gini: 0.259491\n",
      "[200]\tvalid_0's binary_logloss: 0.15245\tvalid_0's gini: 0.275356\n",
      "[300]\tvalid_0's binary_logloss: 0.152023\tvalid_0's gini: 0.283355\n",
      "[400]\tvalid_0's binary_logloss: 0.15182\tvalid_0's gini: 0.287067\n",
      "[500]\tvalid_0's binary_logloss: 0.151724\tvalid_0's gini: 0.288696\n",
      "[600]\tvalid_0's binary_logloss: 0.151698\tvalid_0's gini: 0.289196\n",
      "[700]\tvalid_0's binary_logloss: 0.151695\tvalid_0's gini: 0.289163\n",
      "Early stopping, best iteration is:\n",
      "[608]\tvalid_0's binary_logloss: 0.151696\tvalid_0's gini: 0.2893\n",
      "fold 1 gini coefficient : 0.2892999803480268\n",
      "\n",
      "######################################## fold 2 / fold 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 838\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 199\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153498\tvalid_0's gini: 0.251026\n",
      "[200]\tvalid_0's binary_logloss: 0.152683\tvalid_0's gini: 0.262651\n",
      "[300]\tvalid_0's binary_logloss: 0.152368\tvalid_0's gini: 0.26861\n",
      "[400]\tvalid_0's binary_logloss: 0.152235\tvalid_0's gini: 0.271852\n",
      "[500]\tvalid_0's binary_logloss: 0.152142\tvalid_0's gini: 0.274063\n",
      "[600]\tvalid_0's binary_logloss: 0.15213\tvalid_0's gini: 0.274364\n",
      "[700]\tvalid_0's binary_logloss: 0.152116\tvalid_0's gini: 0.274788\n",
      "[800]\tvalid_0's binary_logloss: 0.152111\tvalid_0's gini: 0.274697\n",
      "Early stopping, best iteration is:\n",
      "[703]\tvalid_0's binary_logloss: 0.152114\tvalid_0's gini: 0.274833\n",
      "fold 2 gini coefficient : 0.2748334131189091\n",
      "\n",
      "######################################## fold 3 / fold 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 842\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 199\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153252\tvalid_0's gini: 0.261804\n",
      "[200]\tvalid_0's binary_logloss: 0.1523\tvalid_0's gini: 0.274\n",
      "[300]\tvalid_0's binary_logloss: 0.151928\tvalid_0's gini: 0.279202\n",
      "[400]\tvalid_0's binary_logloss: 0.151756\tvalid_0's gini: 0.281816\n",
      "[500]\tvalid_0's binary_logloss: 0.151691\tvalid_0's gini: 0.283152\n",
      "[600]\tvalid_0's binary_logloss: 0.151688\tvalid_0's gini: 0.283099\n",
      "Early stopping, best iteration is:\n",
      "[542]\tvalid_0's binary_logloss: 0.151682\tvalid_0's gini: 0.283391\n",
      "fold 3 gini coefficient : 0.28339108737539287\n",
      "\n",
      "######################################## fold 4 / fold 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 199\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153488\tvalid_0's gini: 0.246471\n",
      "[200]\tvalid_0's binary_logloss: 0.152659\tvalid_0's gini: 0.259669\n",
      "[300]\tvalid_0's binary_logloss: 0.152353\tvalid_0's gini: 0.26484\n",
      "[400]\tvalid_0's binary_logloss: 0.152231\tvalid_0's gini: 0.267261\n",
      "[500]\tvalid_0's binary_logloss: 0.152185\tvalid_0's gini: 0.268065\n",
      "[600]\tvalid_0's binary_logloss: 0.152172\tvalid_0's gini: 0.268316\n",
      "[700]\tvalid_0's binary_logloss: 0.152172\tvalid_0's gini: 0.268285\n",
      "Early stopping, best iteration is:\n",
      "[642]\tvalid_0's binary_logloss: 0.152166\tvalid_0's gini: 0.26844\n",
      "fold 4 gini coefficient : 0.26844020025167287\n",
      "\n",
      "######################################## fold 5 / fold 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 843\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 199\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153455\tvalid_0's gini: 0.263044\n",
      "[200]\tvalid_0's binary_logloss: 0.152588\tvalid_0's gini: 0.274863\n",
      "[300]\tvalid_0's binary_logloss: 0.152214\tvalid_0's gini: 0.281735\n",
      "[400]\tvalid_0's binary_logloss: 0.152018\tvalid_0's gini: 0.286393\n",
      "[500]\tvalid_0's binary_logloss: 0.151924\tvalid_0's gini: 0.288675\n",
      "[600]\tvalid_0's binary_logloss: 0.151908\tvalid_0's gini: 0.289154\n",
      "[700]\tvalid_0's binary_logloss: 0.151902\tvalid_0's gini: 0.289013\n",
      "Early stopping, best iteration is:\n",
      "[679]\tvalid_0's binary_logloss: 0.151897\tvalid_0's gini: 0.289271\n",
      "fold 5 gini coefficient : 0.28927137710765916\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "from lightgbm import log_evaluation\n",
    "\n",
    "# Train, validate, and predict models in an OOF way\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # Output a phrase that identifies each fold\n",
    "    print('#'*40, f'fold {idx+1} / fold {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    # Train data\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    # Validation data\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "\n",
    "    # Convert them to LightGBM dataset\n",
    "    dtrain = lgb.Dataset(X_train, y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid)\n",
    "\n",
    "\n",
    "    lgb_model = lgb.train(params=params,\n",
    "                          train_set=dtrain,\n",
    "                          num_boost_round=1000,\n",
    "                          valid_sets=dvalid,\n",
    "                          feval=gini,\n",
    "                          callbacks=[early_stopping(stopping_rounds=100),\n",
    "                                     log_evaluation(100)])\n",
    "    \n",
    "    # OOF prediction using test data\n",
    "    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n",
    "    \n",
    "    # Prediction of validation data target value for model performance evaluation\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "    \n",
    "    # Normalized Gini coefficient for prediction probability of validation data\n",
    "    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'fold {idx+1} gini coefficient : {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF validation prediction gini coefficient: 0.2808785337196325\n"
     ]
    }
   ],
   "source": [
    "print('OOF validation prediction gini coefficient:', eval_gini(y, oof_val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
